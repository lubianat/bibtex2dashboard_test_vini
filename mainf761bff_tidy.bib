%
% This is file `coppe.bib'.
%
% Bibliographic references for the documentation.
%
% Copyright (C) 2011 CoppeTeX Project and any individual authors listed
% elsewhere in this file.
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License version 3 as
% published by the Free Software Foundation.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License version 3 for more details.
%
% You should have received a copy of the GNU General Public License
% version 3 along with this package (see COPYING file).
% If not, see <http://www.gnu.org/licenses/>.
%
% $URL: https://coppetex.svn.sourceforge.net/svnroot/coppetex/trunk/coppe.bib $
% $Id: coppe.bib 118 2008-10-18 14:17:06Z helano $
%
% Author(s): Vicente H. F. Batista
%            George O. Ainsworth Jr.
%

% Custom
@misc{trung_2020,
	title        = {PROV v2.0.0: A Python library for W3C Provenance Data Model},
	author       = {Trung Dong},
	year         = 2020,
	month        = nov,
	journal      = {GitHub},
	url          = {https://github.com/trungdong/prov}
}
@misc{dfanalyzer-spark,
	title        = {DfAnalyzer-Spark},
	author       = {Silva, Vitor},
	year         = 2018,
	month        = mar,
	journal      = {GitHub},
	url          = {https://github.com/vssousa/dfanalyzer-spark}
}
@misc{siemens_2020,
	title        = {TinyDB v4.3.0: a lightweight document oriented database},
	author       = {Markus Siemens},
	year         = 2020,
	month        = nov,
	journal      = {GitHub},
	url          = {https://github.com/msiemens/tinydb}
}
@article{Goncalves2012,
	title        = {{Using domain-specific data to enhance scientific workflow steering queries}},
	author       = {Gon{\c{c}}alves, Jo{\~{a}}o Carlos De A.R. and {De Oliveira}, Daniel and Oca{\~{n}}a, Kary A.C.S. and Ogasawara, Eduardo and Mattoso, Marta},
	year         = 2012,
	journal      = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume       = {7525 LNCS},
	pages        = {152--167},
	doi          = {10.1007/978-3-642-34222-6_12},
	isbn         = 9783642342219,
	issn         = {03029743},
	abstract     = {In scientific workflows, provenance data helps scientists in understanding, evaluating and reproducing their results. Provenance data generated at runtime can also support workflow steering mechanisms. Steering facilities for workflows is considered a challenge due to its dynamic demands during execution. To steer, for example, scientists should be able to suspend (or stop) a workflow execution when the approximate solution meets (or deviates) preset criteria. These criteria are commonly evaluated based on provenance data (execution data) and domain-specific data. We claim that the final decision on whether to interfere on the workflow execution may only become feasible when workflows can be steered by scientists using provenance data enriched with domain-specific data. In this paper we propose an approach based on specialized software components, named Data Extractor (DE), to acquire domain-specific data from data files produced during a scientific workflow execution. DE gathers domain-specific data from produced data files and associates it to existing provenance data on the provenance repository. We have evaluated the proposed approach using a real bioinformatics workflow for comparative genomics executed in SciCumulus cloud workflow parallel engine. {\textcopyright} 2012 Springer-Verlag.}
}
@book{Holmes2018,
	title        = {{Modern Statistics for Modern Biology}},
	author       = {Holmes, Susan and Huber, Wolfgang},
	year         = 2018,
	url          = {http://web.stanford.edu/class/bios221/book/}
}
@article{Moreau2014,
	title        = {{PROV-DM: The PROV Data Model}},
	author       = {Moreau, Luc and Gil, Yolanda and Lebo, Timothy and Mccusker, Jim},
	year         = 2014,
	number       = {April 2013},
	pages        = {1--38},
	url          = {http://www.w3.org/TR/2013/REC-prov-dm-20130430/{\%}5Cnhttp://www.w3.org/TR/prov-dm/{\%}5Cnhttp://www.w3.org/TR/2013/NOTE-prov-implementations-20130430/{\%}5Cnhttp://www.w3.org/TR/2013/PR-prov-dm-20130312/},
	abstract     = {Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3C provenance (PROV) family of specifications. PROV-DM distinguishes core structures, forming the essence of provenance information, from extended structures catering for more specific uses of provenance. PROV-DM is organized in six components, respectively dealing with: (1) entities and activities, and the time at which they were created, used, or ended; (2) derivations of entities from entities; (3) agents bearing responsibility for entities that were generated and activities that happened; (4) a notion of bundle, a mechanism to support provenance of provenance; (5) properties to link entities that refer to the same thing; and, (6) collections forming a logical structure for its members. This document introduces the provenance concepts found in PROV and defines PROV-DM types and relations. The PROV data model is domain-agnostic, but is equipped with extensibility points allowing domain-specific information to be included. Two further documents complete the specification of PROV-DM. First, a companion document specifies the set of constraints that provenance should follow. Second, a separate document describes a provenance notation for expressing instances of provenance for human consumption; this notation is used in examples in this document.}
}
@article{Koster2012,
	title        = {{Snakemake-a scalable bioinformatics workflow engine}},
	author       = {K{\"{o}}ster, Johannes and Rahmann, Sven},
	year         = 2012,
	journal      = {Bioinformatics},
	volume       = 28,
	number       = 19,
	pages        = {2520--2522},
	doi          = {10.1093/bioinformatics/bts480},
	issn         = 14602059,
	abstract     = {Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames. {\textcopyright} The Author 2012. Published by Oxford University Press. All rights reserved.},
	pmid         = 29788404
}
@article{Virtanen2020,
	title        = {{SciPy 1.0: fundamental algorithms for scientific computing in Python}},
	author       = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'{e}}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R.J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^{o}}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"{a}}ggstr{\"{o}}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'{e}} and Probst, Irvin and Dietrich, J{\"{o}}rg P. and Silterra, Jacob and Webber, James T. and Slavi{\v{c}}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"{o}}nberger, Johannes L. and {de Miranda Cardoso}, Jos{\'{e}} Vin{\'{i}}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'{i}}guez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"{u}}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and V{\'{a}}zquez-Baeza, Yoshiki},
	year         = 2020,
	journal      = {Nature Methods},
	doi          = {10.1038/s41592-019-0686-2},
	issn         = 15487105,
	abstract     = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	archiveprefix = {arXiv},
	arxivid      = {1907.10121},
	eprint       = {1907.10121},
	pmid         = 32015543
}
@article{perkel2018jupyter,
	title        = {{Why Jupyter is data scientists' computational notebook of choice}},
	author       = {Perkel, Jeffrey M},
	year         = 2018,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 563,
	number       = 7732,
	pages        = {145--147}
}
@misc{Oliphant2006,
	title        = {{A guide to NumPy}},
	author       = {Oliphant, Travis},
	year         = 2006,
	booktitle    = {Trelgol Publishing},
	doi          = {DOI:10.1109/MCSE.2007.58},
	abstract     = {By itself, Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.}
}
@phdthesis{Campos2018,
	title        = {{DfA-lib-Python: uma biblioteca para extra{\c{c}}{\~{a}}o de dados cient{\'{i}}ficos usando a DfAnalyzer}},
	author       = {Campos, Vinicius Silva},
	year         = 2018,
	pages        = {1--45},
	school       = {Universidade Federal do Rio de Janeiro}
}
@article{McKinney2011,
	title        = {{pandas: a Foundational Python Library for Data Analysis and Statistics}},
	author       = {McKinney, Wes},
	year         = 2011,
	journal      = {Python for High Performance and Scientific Computing},
	abstract     = {In this paper we will discuss pandas, a Python library of rich data structures and tools for working with structured data sets common to statistics, ﬁnance, social sciences, and many other ﬁelds. The library provides integrated, intuitive routines for performing common data manipulations and analysis on such data sets. It aims to be the foundational layer for the future of statistical computing in Python. It serves as a strong complement to the existing scientiﬁc Python stack while implementing and improving upon the kinds of data manipulation tools found in other statistical programming languages such as R. In addition to detailing its design and features of pandas, we will discuss future avenues of work and growth opportunities for statistics and data analysis applications in the Python language.}
}
@article{Cock2009,
	title        = {{Biopython: freely available Python tools for computational molecular biology and bioinformatics}},
	author       = {Cock, P. J. A. and Antao, Tiago and Chang, Jeffrey T. and Chapman, Brad A. and Cox, Cymon J. and Dalke, Andrew and Friedberg, Iddo and Hamelryck, Thomas and Kauff, Frank and Wilczynski, Bartek and de Hoon, M. J. L.},
	year         = 2009,
	month        = jun,
	journal      = {Bioinformatics},
	volume       = 25,
	number       = 11,
	pages        = {1422--1423},
	doi          = {10.1093/bioinformatics/btp163},
	issn         = {1367-4803},
	url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btp163},
	abstract     = {Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. {\textcopyright} The Author 2009. Published by Oxford University Press. All rights reserved.}
}
@inproceedings{biojulia2020,
	title        = {{BioJulia}},
	year         = 2020,
	url          = {https://biojulia.net/}
}
@article{Koster2016,
	title        = {{Rust-Bio: a fast and safe bioinformatics library}},
	author       = {K{\"{o}}ster, Johannes},
	year         = 2015,
	journal      = {Bioinformatics},
	volume       = 32,
	number       = 3,
	pages        = {444--446},
	doi          = {10.1093/bioinformatics/btv573},
	issn         = {1367-4803},
	url          = {https://doi.org/10.1093/bioinformatics/btv573},
	abstract     = {Summary: We present Rust-Bio, the first general purpose bioinformatics library for the innovative Rust programming language. Rust-Bio leverages the unique combination of speed, memory safety and high-level syntax offered by Rust to provide a fast and safe set of bioinformatics algorithms and data structures with a focus on sequence analysis.Availability and implementation: Rust-Bio is available open source under the MIT license at https://rust-bio.github.io.Contact:koester@jimmy.harvard.eduSupplementary information:Supplementary data are available at Bioinformatics online.}
}
@article{holland2008biojava,
	title        = {BioJava: an open-source framework for bioinformatics},
	author       = {Holland, Richard CG and Down, Thomas A and Pocock, Matthew and Prli{\'c}, Andreas and Huen, David and James, Keith and Foisy, Sylvain and Dr{\"a}ger, Andreas and Yates, Andy and Heuer, Michael and others},
	year         = 2008,
	journal      = {Bioinformatics},
	publisher    = {Oxford University Press},
	volume       = 24,
	number       = 18,
	pages        = {2096--2097}
}
@phdthesis{DaCruz2011,
	title        = {{Uma Estrat{\'{e}}gia de Apoio {\`{a}} Ger{\^{e}}ncia de Dados de Proveni{\^{e}}ncia em Experimentos Cient{\'{i}}ficos}},
	author       = {{Da Cruz}, S{\'{e}}rgio Manuel Serra},
	year         = 2011,
	school       = {UFRJ}
}
@article{Hidalgo2018,
	title        = {{Bioinformatics software for genomic: a systematic review on GitHub}},
	author       = {Hidalgo, Carlos Giovanny and Guevara, Miguel Eduardo and Bucheli, Victor Andres and Moreno, Pedro Antonio},
	year         = 2018,
	journal      = {PeerJ Preprints},
	pages        = {1--14},
	doi          = {10.7287/peerj.preprints.27352},
	issn         = {2167-9843},
	url          = {https://doi.org/10.7287/peerj.preprints.27352v3},
	keywords     = {Bioinformatics,GitHub,Mining Software Repositories,State-of-the-art technique,open source genomics software}
}
@inproceedings{Ferme2016,
	title        = {{A container-centric methodology for benchmarking workflow management systems}},
	author       = {Ferme, Vincenzo and Ivanchikj, Ana and Pautasso, Cesare and Skouradaki, Marigianna and Leymann, Frank},
	year         = 2016,
	booktitle    = {CLOSER 2016 - Proceedings of the 6th International Conference on Cloud Computing and Services Science},
	publisher    = {SciTePress},
	volume       = 2,
	pages        = {74--84},
	doi          = {10.5220/0005908400740084},
	isbn         = 9789897581823,
	abstract     = {Trusted benchmarks should provide reproducible results obtained following a transparent and well-defined process. In this paper, we show how Containers, originally developed to ease the automated deployment of Cloud application components, can be used in the context of a benchmarking methodology. The proposed methodology focuses on Workflow Management Systems (WfMSs), a critical service orchestration middleware, which can be characterized by its architectural complexity, for which Docker Containers offer a highly suitable approach. The contributions of our work are: 1) a new benchmarking approach taking full advantage of containerization technologies; and 2) the formalization of the interaction process with the WfMS vendors described clearly in a written agreement. Thus, we take advantage of emerging Cloud technologies to address technical challenges, ensuring the performance measurements can be trusted. We also make the benchmarking process transparent, automated, and repeatable so that WfMS vendors can join the benchmarking effort.},
	keywords     = {Benchmarking,Cloud applications,Docker containers,Workflow management systems}
}
@article{Gruening2019,
	title        = {{Recommendations for the packaging and containerizing of bioinformatics software}},
	author       = {Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and {da Veiga Leprevost}, Felipe and M{\'{e}}nager, Herv{\'{e}} and S{\o}ndergaard, Dan and R{\"{o}}st, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, F{\'{a}}bio and {Dominguez Del Angel}, Victoria and Crusoe, Michael R. and Varma, Susheel and Blankenberg, Daniel and Jimenez, Rafael C. and Perez-Riverol, Yasset},
	year         = 2019,
	month        = mar,
	journal      = {F1000Research},
	publisher    = {F1000 Research Ltd},
	volume       = 7,
	pages        = 742,
	doi          = {10.12688/f1000research.15140.2},
	issn         = {2046-1402},
	url          = {https://f1000research.com/articles/7-742/v2},
	abstract     = {Software Containers are changing the way scientists and researchers develop, deploy and exchange scientific software. They allow labs of all sizes to easily install bioinformatics software, maintain multiple versions of the same software and combine tools into powerful analysis pipelines. However, containers and software packages should be produced under certain rules and standards in order to be reusable, compatible and easy to integrate into pipelines and analysis workflows. Here, we presented a set of recommendations developed by the BioContainers Community to produce standardized bioinformatics packages and containers. These recommendations provide practical guidelines to make bioinformatics software more discoverable, reusable and transparent. They are aimed to guide developers, organisations, journals and funders to increase the quality and sustainability of research software.},
	keywords     = {Best practices bioinformatics,Containers and packages,Reproducibility}
}
@article{Kanwal2017,
	title        = {{Investigating reproducibility and tracking provenance - A genomic workflow case study}},
	author       = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O.},
	year         = 2017,
	month        = jul,
	journal      = {BMC Bioinformatics},
	publisher    = {BioMed Central Ltd.},
	volume       = 18,
	number       = 1,
	pages        = 337,
	doi          = {10.1186/s12859-017-1747-0},
	issn         = 14712105,
	url          = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1747-0},
	abstract     = {Background: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. Results: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. Conclusions: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
	keywords     = {Common Workflow Language (CWL),Cpipe,Galaxy,Provenance,Reproducibility,Workflow}
}
@article{DePaula2013,
	title        = {{Provenance in bioinformatics workflows.}},
	author       = {de Paula, Renato and Holanda, Maristela and Gomes, Luciana S.A. and Lifschitz, Sergio and Walter, Maria Emilia M.T.},
	year         = 2013,
	journal      = {BMC bioinformatics},
	publisher    = {BioMed Central},
	volume       = {14 Suppl 1},
	number       = {Suppl 11},
	pages        = {S6},
	doi          = {10.1186/1471-2105-14-S11-S6},
	issn         = 14712105,
	abstract     = {In this work, we used the PROV-DM model to manage data provenance in workflows of genome projects. This provenance model allows the storage of details of one workflow execution, e.g., raw and produced data and computational tools, their versions and parameters. Using this model, biologists can access details of one particular execution of a workflow, compare results produced by different executions, and plan new experiments more efficiently. In addition to this, a provenance simulator was created, which facilitates the inclusion of provenance data of one genome project workflow execution. Finally, we discuss one case study, which aims to identify genes involved in specific metabolic pathways of Bacillus cereus, as well as to compare this isolate with other phylogenetic related bacteria from the Bacillus group. B. cereus is an extremophilic bacteria, collected in warm water in the Midwestern Region of Brazil, its DNA samples having been sequenced with an NGS machine.}
}
@misc{Ewels2020,
	title        = {{The nf-core framework for community-curated bioinformatics pipelines}},
	author       = {Ewels, Philip A. and Peltzer, Alexander and Fillinger, Sven and Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and Garcia, Maxime Ulysse and {Di Tommaso}, Paolo and Nahnsen, Sven},
	year         = 2020,
	month        = feb,
	booktitle    = {Nature Biotechnology},
	publisher    = {Nature Research},
	pages        = {1--3},
	doi          = {10.1038/s41587-020-0439-x},
	issn         = 15461696,
	keywords     = {Computational biology and bioinformatics,Scientific community}
}
@article{DiTommaso2015,
	title        = {{The impact of Docker containers on the performance of genomic pipelines}},
	author       = {{Di Tommaso}, Paolo and Palumbo, Emilio and Chatzou, Maria and Prieto, Pablo and Heuer, Michael L. and Notredame, Cedric},
	year         = 2015,
	journal      = {PeerJ},
	volume       = 2015,
	number       = 9,
	pages        = {1--10},
	doi          = {10.7717/peerj.1273},
	issn         = 21678359,
	abstract     = {Genomic pipelines consist of several pieces of third party software and, because of their experimental nature, frequent changes and updates are commonly necessary thus raising serious deployment and reproducibility issues. Docker containers are emerging as a possible solution for many of these problems, as they allow the packaging of pipelines in an isolated and self-contained manner. This makes it easy to distribute and execute pipelines in a portable manner across a wide range of computing platforms. Thus, the question that arises is to what extent the use of Docker containers might affect the performance of these pipelines. Here we address this question and conclude that Docker containers have only a minor impact on the performance of common genomic pipelines, which is negligible when the executed jobs are long in terms of computational time.},
	keywords     = {Bioinformatics,Docker,Pipelines,Virtualisation,Workflow}
}
@article{Lampa2019,
	title        = {{Scipipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines}},
	author       = {Lampa, Samuel and Dahl{\"{o}}, Martin and Alvarsson, Jonathan and Spjuth, Ola},
	year         = 2019,
	journal      = {GigaScience},
	volume       = 8,
	number       = 5,
	doi          = {10.1093/gigascience/giz044},
	issn         = {2047217X},
	abstract     = {Background: The complex nature of biological data has driven the development of specialized software tools. Scientific workflow management systems simplify the assembly of such tools into pipelines, assist with job automation, and aid reproducibility of analyses. Many contemporary workflow tools are specialized or not designed for highly complex workflows, such as with nested loops, dynamic scheduling, and parametrization, which is common in, e.g., machine learning. Findings: SciPipe is a workflow programming library implemented in the programming language Go, for managing complex and dynamic pipelines in bioinformatics, cheminformatics, and other fields. SciPipe helps in particular with workflow constructs common in machine learning, such as extensive branching, parameter sweeps, and dynamic scheduling and parametrization of downstream tasks. SciPipe builds on flow-based programming principles to support agile development of workflows based on a library of self-contained, reusable components. It supports running subsets of workflows for improved iterative development and provides a data-centric audit logging feature that saves a full audit trace for every output file of a workflow, which can be converted to other formats such as HTML, TeX, and PDF on demand. The utility of SciPipe is demonstrated with a machine learning pipeline, a genomics, and a transcriptomics pipeline. Conclusions: SciPipe provides a solution for agile development of complex and dynamic pipelines, especially in machine learning, through a flexible application programming interface suitable for scientists used to programming or scripting.},
	keywords     = {Flow-based programming,Go,Golang,Machine learning,Pipelines,Reproducibility,Scientific workflow management systems}
}
@article{Fjukstad2017,
	title        = {{A Review of Scalable Bioinformatics Pipelines}},
	author       = {Fjukstad, Bj{\o}rn and Bongo, Lars Ailo},
	year         = 2017,
	journal      = {Data Science and Engineering},
	publisher    = {Springer Berlin Heidelberg},
	volume       = 2,
	number       = 3,
	pages        = {245--251},
	doi          = {10.1007/s41019-017-0047-z},
	issn         = 23641541,
	abstract     = {Scalability is increasingly important for bioinformatics analysis services, since these must handle larger datasets, more jobs, and more users. The pipelines used to implement analyses must therefore scale with respect to the resources on a single compute node, the number of nodes on a cluster, and also to cost-performance. Here, we survey several scalable bioinformatics pipelines and compare their design and their use of underlying frameworks and infrastructures. We also discuss current trends for bioinformatics pipeline development.},
	keywords     = {Analysis services,Bioinformatics,Infrastructure,Pipeline,Scalable}
}
@article{DITommaso2017,
	title        = {{Nextflow enables reproducible computational workflows}},
	author       = {{DI Tommaso}, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
	year         = 2017,
	journal      = {Nature Biotechnology},
	volume       = 35,
	number       = 4,
	pages        = {316--319},
	doi          = {10.1038/nbt.3820},
	issn         = 15461696,
	abstract     = {In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. The central contribution is an illumination invariant, which we show to be suitable for recognition from video of loosely constrained head motion. In particular there are three contributions: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation to exploit the proposed invariant and generalize in the presence of extreme illumination changes; (ii) we introduce a video sequence re-illumination algorithm to achieve fine alignment of two video sequences; and (iii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve robustness to unseen head poses. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 323 individuals and 1474 video sequences with extreme illumination, pose and head motion variation. Our system consistently achieved a nearly perfect recognition rate (over 99.7{\%} on all four databases). ?? 2012 Elsevier Ltd All rights reserved.}
}
@article{Marx2013,
	title        = {{Biology: The big challenges of big data}},
	author       = {Marx, Vivien},
	year         = 2013,
	month        = jun,
	journal      = {Nature},
	publisher    = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	volume       = 498,
	number       = 7453,
	pages        = 255,
	doi          = {10.1038/498255a},
	issn         = {00280836},
	url          = {https://doi.org/10.1038/498255a http://10.0.4.14/498255a},
	abstract     = {As they grapple with increasingly large data sets, biologists and computer scientists uncork new bottlenecks.}
}
@article{Stephens2015,
	title        = {{Big data: Astronomical or genomical?}},
	author       = {Stephens, Zachary D. and Lee, Skylar Y. and Faghri, Faraz and Campbell, Roy H. and Zhai, Chengxiang and Efron, Miles J. and Iyer, Ravishankar and Schatz, Michael C. and Sinha, Saurabh and Robinson, Gene E.},
	year         = 2015,
	journal      = {PLoS Biology},
	volume       = 13,
	number       = 7,
	pages        = {1--11},
	doi          = {10.1371/journal.pbio.1002195},
	issn         = 15457885,
	abstract     = {Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, communitywide planning for the “genomical” challenges of the next decade.}
}
@inproceedings{ragan2014jupyter,
	title        = {{The Jupyter/IPython architecture: a unified view of computational research, from interactive exploration to communication and publication.}},
	author       = {Ragan-Kelley, Min and Perez, F and Granger, B and Kluyver, T and Ivanov, P and Frederic, J and Bussonnier, M},
	year         = 2014,
	booktitle    = {AGU Fall Meeting Abstracts}
}
@inproceedings{costa2013capturing,
	title        = {{Capturing and querying workflow runtime provenance with PROV: a practical approach}},
	author       = {Costa, Flavio and Silva, V{\'{i}}tor and {De Oliveira}, Daniel and Oca{\~{n}}a, Kary and Ogasawara, Eduardo and Dias, Jonas and Mattoso, Marta},
	year         = 2013,
	booktitle    = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
	pages        = {282--289},
	organization = {ACM}
}
@article{craddock2008science,
	title        = {{e-Science: relieving bottlenecks in large-scale genome analyses}},
	author       = {Craddock, Tracy and Harwood, Colin R and Hallinan, Jennifer and Wipat, Anil},
	year         = 2008,
	journal      = {Nature reviews microbiology},
	publisher    = {Nature Publishing Group},
	volume       = 6,
	number       = 12,
	pages        = 948
}
@article{Leipzig2018,
	title        = {{A review of bioinformatic pipeline frameworks}},
	author       = {Leipzig, Jeremy},
	year         = 2016,
	journal      = {Briefings in Bioinformatics},
	volume       = 18,
	number       = 3,
	pages        = {530--536},
	doi          = {10.1093/bib/bbw020},
	issn         = {1477-4054},
	url          = {https://doi.org/10.1093/bib/bbw020},
	abstract     = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base.}
}
@article{wilson2017good,
	title        = {{Good enough practices in scientific computing}},
	author       = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K},
	year         = 2017,
	month        = jun,
	journal      = {PLOS Computational Biology},
	publisher    = {Public Library of Science},
	volume       = 13,
	number       = 6,
	pages        = {e1005510},
	doi          = {10.1371/journal.pcbi.1005510},
	issn         = {1553-7358},
	url          = {https://dx.plos.org/10.1371/journal.pcbi.1005510},
	editor       = {Ouellette, Francis}
}
@article{wilson2014best,
	title        = {{Best practices for scientific computing}},
	author       = {Wilson, Greg and Aruliah, Dhavide A and Brown, C Titus and Hong, Neil P Chue and Davis, Matt and Guy, Richard T and Haddock, Steven H D and Huff, Kathryn D and Mitchell, Ian M and Plumbley, Mark D and Others},
	year         = 2014,
	journal      = {PLoS Biology},
	publisher    = {Public Library of Science},
	volume       = 12,
	number       = 1,
	pages        = {e1001745}
}
@article{Cheifet2019,
	title        = {{Where is genomics going next?}},
	author       = {Cheifet, Barbara},
	year         = 2019,
	journal      = {Genome Biology},
	publisher    = {Genome Biology},
	volume       = 20,
	number       = 1,
	pages        = {1--8},
	doi          = {10.1186/s13059-019-1626-2},
	isbn         = 1305901916,
	issn         = {1474760X},
	abstract     = {We polled the Editorial Board of Genome Biology to ask where they see genomics going in the next few years. Here are some of their responses.}
}
@article{Pasquier2017,
	title        = {{If these data could talk}},
	author       = {Pasquier, Thomas and Lau, Matthew K. and Trisovic, Ana and Boose, Emery R. and Couturier, Ben and Crosas, Merc{\`{e}} and Ellison, Aaron M. and Gibson, Valerie and Jones, Chris R. and Seltzer, Margo},
	year         = 2017,
	journal      = {Scientific Data},
	volume       = 4,
	pages        = {1--5},
	doi          = {10.1038/sdata.2017.114},
	issn         = 20524463,
	abstract     = {In the last few decades, data-driven methods have come to dominate many fields of scientific inquiry. Open data and open-source software have enabled the rapid implementation of novel methods to manage and analyze the growing flood of data. However, it has become apparent that many scientific fields exhibit distressingly low rates of reproducibility. Although there are many dimensions to this issue, we believe that there is a lack of formalism used when describing end-to-end published results, from the data source to the analysis to the final published results. Even when authors do their best to make their research and data accessible, this lack of formalism reduces the clarity and efficiency of reporting, which contributes to issues of reproducibility. Data provenance aids both reproducibility through systematic and formal records of the relationships among data sources, processes, datasets, publications and researchers. Reproducibility}
}
@article{Yuan2018,
	title        = {{Utilizing Provenance in Reusable Research Objects}},
	author       = {Yuan, Zhihao and {Ton That}, Dai and Kothari, Siddhant and Fils, Gabriel and Malik, Tanu},
	year         = 2018,
	journal      = {Informatics},
	volume       = 5,
	number       = 1,
	pages        = 14,
	doi          = {10.3390/informatics5010014},
	issn         = 22279709,
	abstract     = {Science is conducted collaboratively, often requiring the sharing of knowledge about computational experiments. When experiments include only datasets, they can be shared using Uniform Resource Identifiers (URIs) or Digital Object Identifiers (DOIs). An experiment, however, seldom includes only datasets, but more often includes software, its past execution, provenance, and associated documentation. The Research Object has recently emerged as a comprehensive and systematic method for aggregation and identification of diverse elements of computational experiments. While a necessary method, mere aggregation is not sufficient for the sharing of computational experiments. Other users must be able to easily recompute on these shared research objects. Computational provenance is often the key to enable such reuse. In this paper, we show how reusable research objects can utilize provenance to correctly repeat a previous reference execution, to construct a subset of a research object for partial reuse, and to reuse existing contents of a research object for modified reuse. We describe two methods to summarize provenance that aid in understanding the contents and past executions of a research object. The first method obtains a process-view by collapsing low-level system information, and the second method obtains a summary graph by grouping related nodes and edges with the goal to obtain a graph view similar to application workflow. Through detailed experiments, we show the efficacy and efficiency of our algorithms.},
	keywords     = {interactive reproducibility,provenance graph,reproducibility,reusable research object,summarization graph}
}
@article{Stevens2007,
	title        = {{Using provenance to manage knowledge of In Silico experiments}},
	author       = {Stevens, Robert and Zhao, Jun and Goble, Carole},
	year         = 2007,
	journal      = {Briefings in Bioinformatics},
	volume       = 8,
	number       = 3,
	pages        = {183--194},
	doi          = {10.1093/bib/bbm015},
	issn         = 14675463,
	abstract     = {This article offers a briefing in one of the knowledge management issues of in silico experimentation in bioinformatics. Recording of the provenance of an experiment-what was done; where, how and why, etc. is an important aspect of scientific best practice that should be extended to in silico experimentation. We will do this in the context of eScience which has been part of the move of bioinformatics towards an industrial setting. Despite the computational nature of bioinformatics, these analyses are scientific and thus necessitate their own versions of typical scientific rigour. Just as recording who, what, why, when, where and how of an experiment is central to the scientific process in laboratory science, so it should be in silico science. The generation and recording of these aspects, or provenance, of an experiment are necessary knowledge management goals if we are to introduce scientific rigour into routine bioinformatics. In Silico experimental protocols should themselves be a form of managing the knowledge of how to perform bioinformatics analyses. Several systems now exist that offer support for the generation and collection of provenance information about how a particular in silico experiment was run, what results were generated, how they were generated, etc. In reviewing provenance support, we will review one of the important knowledge management issues in bioinformatics.},
	keywords     = {Data derivation,In Silico experiments,Provenance,Validation and verification of results,Workflow}
}
@article{Mattoso2010,
	title        = {{Towards supporting the life cycle of large scale scientific experiments}},
	author       = {Mattoso, Marta and Werner, Claudia and Travassos, Guilherme Horta and Braganholo, Vanessa and Ogasawara, Eduardo and Oliveira, Daniel De and Cruz, Sergio Manuel Serra Da and Martinho, Wallace and Murta, Leonardo},
	year         = 2010,
	journal      = {International Journal of Business Process Integration and Management},
	volume       = 5,
	number       = 1,
	pages        = 79,
	doi          = {10.1504/ijbpim.2010.033176},
	issn         = {1741-8763},
	abstract     = {One of the main challenges of scientific experiments is to allow scientists to manage and exchange their scientific computational resources (data, programs, models, etc.). The effective management of such experiments requires a specific set of cardinal facilities, such as experiment specification techniques, workflow derivation heuristics and provenance mechanisms. These facilities may characterise the experiment life cycle into three phases: composition, execution, and analysis. Works concerned with supporting scientific workflows are mainly concerned with the execution and analysis phase. Therefore, they fail to support the scientific experiment throughout its life cycle as a set of integrated experimentation technologies. In large scale experiments this represents a research challenge. We propose an approach for managing large scale experiments based on provenance gathering during all phases of the life cycle. We foresee that such approach may aid scientists to have more control on the trials of the scientific experiment.}
}
@article{Markowetz2017,
	title        = {{All biology is computational biology}},
	author       = {Markowetz, Florian},
	year         = 2017,
	journal      = {PLoS Biology},
	volume       = 15,
	number       = 3,
	pages        = {4--7},
	doi          = {10.1371/journal.pbio.2002050},
	isbn         = 1111111111,
	issn         = 15457885,
	abstract     = {Here, I argue that computational thinking and techniques are so central to the quest of understanding life that today all biology is computational biology. Computational biology brings order into our understanding of life, it makes biological concepts rigorous and testable, and it provides a reference map that holds together individual insights. The next modern synthesis in biology will be driven by mathematical, statistical, and computational methods being absorbed into mainstream biological training, turning biology into a quantitative science.},
	pmid         = 28278152
}
@article{Silva2018,
	title        = {{DfAnalyzer: Runtime Dataflow Analysis of Scientific Applications using Provenance}},
	author       = {Silva, V{\'{i}}tor and de Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
	year         = 2018,
	volume       = 11,
	number       = 12,
	pages        = {2082--2085},
	doi          = {10.14778/3229863.3236265},
	url          = {https://doi.org/10.14778/3229863.3236265},
	abstract     = {We present DfAnalyzer, a tool that enables monitoring, debugging, steering, and analysis of dataflows while being generated by scientific applications. It works by capturing strategic domain data, registering provenance and execution data to enable queries at runtime. DfAnalyzer provides lightweight dataflow monitoring components to be invoked by high performance applications. It can be plugged in scientific code scripts, or Spark applications, in the same way users already plug visualization library components. During this demo, we will show how DfAnalyzer captures the dataflow, provenance, as well as how it provides runtime data analyses of applications. We will also encourage attendees to use DfAnalyzer for their own applications.}
}
@article{Parks2017,
	title        = {{Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life}},
	author       = {Parks, Donovan H. and Rinke, Christian and Chuvochina, Maria and Chaumeil, Pierre Alain and Woodcroft, Ben J. and Evans, Paul N. and Hugenholtz, Philip and Tyson, Gene W.},
	year         = 2017,
	journal      = {Nature Microbiology},
	publisher    = {Springer US},
	volume       = 2,
	number       = 11,
	pages        = {1533--1542},
	doi          = {10.1038/s41564-017-0012-7},
	isbn         = 4156401700127,
	issn         = 20585276,
	url          = {http://dx.doi.org/10.1038/s41564-017-0012-7},
	abstract     = {Challenges in cultivating microorganisms have limited the phylogenetic diversity of currently available microbial genomes. This is being addressed by advances in sequencing throughput and computational techniques that allow for the cultivation-independent recovery of genomes from metagenomes. Here, we report the reconstruction of 7,903 bacterial and archaeal genomes from {\textgreater}1,500 public metagenomes. All genomes are estimated to be ≥50{\%} complete and nearly half are ≥90{\%} complete with ≤5{\%} contamination. These genomes increase the phylogenetic diversity of bacterial and archaeal genome trees by {\textgreater}30{\%} and provide the first representatives of 17 bacterial and three archaeal candidate phyla. We also recovered 245 genomes from the Patescibacteria superphylum (also known as the Candidate Phyla Radiation) and find that the relative diversity of this group varies substantially with different protein marker sets. The scale and quality of this data set demonstrate that recovering genomes from metagenomes provides an expedient path forward to exploring microbial dark matter.}
}
@article{Nelson2020,
	title        = {{Biases in genome reconstruction from metagenomic data}},
	author       = {Nelson, William C. and Tully, Benjamin J. and Mobberley, Jennifer M.},
	year         = 2020,
	month        = oct,
	journal      = {PeerJ},
	volume       = 8,
	pages        = {e10119},
	doi          = {10.7717/peerj.10119},
	issn         = {2167-8359},
	url          = {https://peerj.com/articles/10119},
	abstract     = {Background: Technological advances in sequencing, assembly and segregation of resulting contigs into species-specific bins has enabled the reconstruction of individual genomes from environmental metagenomic data sets. Though a powerful technique, it is shadowed by an inability to truly determine whether assembly and binning techniques are accurate, specific, and sensitive due to a lack of complete reference genome sequences against which to check the data. Errors in genome reconstruction, such as missing or mis-attributed activities, can have a detrimental effect on downstream metabolic and ecological modeling, and thus it is important to assess the accuracy of the process.$\backslash$n$\backslash$nMethods: We compared genomes reconstructed from metagenomic data to complete genome sequences of 10 organisms isolated from the same community to identify regions not captured by typical binning techniques. The nucleotide content, as {\%}G+C and tetranucleotide frequencies, and sequence redundancy within both the genome and across the metagenome were determined for both the captured and uncaptured regions. This direct comparison allowed us to evaluate the efficacy of nucleotide composition and coverage profiles as elements of binning protocols and look for biases in sequence characteristics and gene content in regions missing from the reconstructions.$\backslash$n$\backslash$nResults: We found that repeated sequences were frequently missed in the reconstruction process as were short sequences with variant nucleotide composition. Genes encoded on the missing regions were strongly biased towards ribosomal RNAs, transfer RNAs, mobile element functions and genes of unknown function.$\backslash$n$\backslash$nConclusions: Our observation of increased mis-binning of short regions, especially those with variant nucleotide content, and repeated regions implies that factors which affect assembly efficiency also impact binning accuracy. To a large extent, mis-binned regions appear to derive from mobile elements. Our results support genome reconstruction as a robust process, and suggest that reconstructions determined to be {\textgreater}90{\%} complete are likely to effectively represent organismal function.},
	keywords     = {Bioinformatics,Genomics,Microbiology,binning,genome reconstruction,genomes from metagenomes,metagenomics}
}
@article{Berube2018,
	title        = {{Data descriptor: Single cell genomes of Prochlorococcus, Synechococcus, and sympatric microbes from diverse marine environments}},
	author       = {Berube, Paul M. and Biller, Steven J. and Hackl, Thomas and Hogle, Shane L. and Satinsky, Brandon M. and Becker, Jamie W. and Braakman, Rogier and Collins, Sara B. and Kelly, Libusha and Berta-Thompson, Jessie and Coe, Allison and Bergauer, Kristin and Bouman, Heather A. and Browning, Thomas J. and {De Corte}, Daniele and Hassler, Christel and Hulata, Yotam and Jacquot, Jeremy E. and Maas, Elizabeth W. and Reinthaler, Thomas and Sintes, Eva and Yokokawa, Taichi and Lindell, Debbie and Stepanauskas, Ramunas and Chisholm, Sallie W.},
	year         = 2018,
	journal      = {Scientific Data},
	volume       = 5,
	number       = {March},
	pages        = {1--11},
	doi          = {10.1038/sdata.2018.154},
	issn         = 20524463,
	abstract     = {Single cell genomes of Prochlorococcus, Synechococcus, and sympatric microbes from diverse marine environments}
}
@article{Rossello-Mora2017,
	title        = {{Introducing a digital protologue: a timely move towards a database-driven systematics of archaea and bacteria}},
	author       = {Rossell{\'{o}}-M{\'{o}}ra, Ramon and Trujillo, Martha E. and Sutcliffe, Iain C.},
	year         = 2017,
	journal      = {Antonie van Leeuwenhoek, International Journal of General and Molecular Microbiology},
	volume       = 110,
	number       = 4,
	pages        = {455--456},
	doi          = {10.1007/s10482-017-0841-7},
	isbn         = 1048201708,
	issn         = 15729699
}
@article{Stackebrandt2019,
	title        = {{Paradigm shift in species description: the need to move towards a tabular format}},
	author       = {Stackebrandt, Erko and Smith, David},
	year         = 2019,
	journal      = {Archives of Microbiology},
	publisher    = {Springer Berlin Heidelberg},
	volume       = 201,
	number       = 2,
	pages        = {143--145},
	doi          = {10.1007/s00203-018-1609-9},
	isbn         = {0123456789},
	issn         = {1432072X},
	url          = {http://dx.doi.org/10.1007/s00203-018-1609-9}
}
@article{Rossello-Mora2019,
	title        = {{Dialogue on the nomenclature and classification of prokaryotes}},
	author       = {Rossell{\'{o}}-M{\'{o}}ra, Ramon and Whitman, William B.},
	year         = 2019,
	journal      = {Systematic and Applied Microbiology},
	publisher    = {Elsevier GmbH.},
	volume       = 42,
	number       = 1,
	pages        = {5--14},
	doi          = {10.1016/j.syapm.2018.07.002},
	issn         = 16180984,
	url          = {https://doi.org/10.1016/j.syapm.2018.07.002},
	abstract     = {The application of next generation sequencing and molecular ecology to the systematics and taxonomy of prokaryotes offers enormous insights into prokaryotic biology. This discussion explores some major disagreements but also considers the opportunities associated with the nomenclature of the uncultured taxa, the use of genome sequences as type material, the plurality of the nomenclatural code, and the roles of an official or computer-assisted taxonomy.},
	keywords     = {Bacteriological code,Candidatus,Nomenclature,Taxonomy,Type material}
}
@article{Salazar2020,
	title        = {{A new genomic taxonomy system for the Synechococcus collective}},
	author       = {Salazar, Vin{\'{i}}cius W and Tschoeke, Diogo A and Swings, Jean and Cosenza, Carlos A and Mattoso, Marta and Thompson, Cristiane C. and Thompson, Fabiano L},
	year         = 2020,
	month        = aug,
	journal      = {Environmental Microbiology},
	pages        = {1462--2920.15173},
	doi          = {10.1111/1462-2920.15173},
	issn         = {1462-2912},
	url          = {https://sfamjournals.onlinelibrary.wiley.com/doi/abs/10.1111/1462-2920.15173 https://onlinelibrary.wiley.com/doi/abs/10.1111/1462-2920.15173},
	abstract     = {Summary Cyanobacteria of the genus Synechococcus are major contributors to global primary productivity and are found in a wide range of aquatic ecosystems. This Synechococcus collective (SC) is metabolically diverse, with some lineages thriving in polar and nutrient-rich locations and others in tropical or riverine waters. Although many studies have discussed the ecology and evolution of the SC, there is a paucity of knowledge on its taxonomic structure. Thus, we present a new taxonomic classification framework for the SC based on recent advances in microbial genomic taxonomy. Phylogenomic analyses of 1085 cyanobacterial genomes demonstrate that organisms classified as Synechococcus are polyphyletic at the order rank. The SC is classified into 15 genera, which are placed into five distinct orders within the phylum Cyanobacteria: i. Synechococcales (Cyanobium, Inmanicoccus, Lacustricoccus gen. nov., Parasynechococcus, Pseudosynechococcus, Regnicoccus, Synechospongium gen. nov., Synechococcus, and Vulcanococcus); ii. Cyanobacteriales (Limnothrix); iii. Leptococcales (Brevicoccus and Leptococcus); iv. Thermosynechococcales (Stenotopis and Thermosynechococcus), and v. Neosynechococcales (Neosynechococcus). The newly proposed classification is consistent with habitat distribution patterns (seawater, freshwater, brackish, and thermal environments) and reflects the ecological and evolutionary relationships of the SC. This article is protected by copyright. All rights reserved.}
}
@article{Tschoeke2020,
	title        = {{Unlocking the genomic taxonomy of the Prochlorococcus collective}},
	author       = {Tschoeke, Diogo A. and Salazar, Vin{\'{i}}cius and Vidal, Livia M. and Campe{\~{a}}o, Mariana and Swings, Jean and Thompson, Fabiano L. and Thompson, Cristiane C.},
	year         = 2020,
	month        = mar,
	journal      = {Microbial Ecology},
	publisher    = {Elsevier},
	pages        = {1--13},
	doi          = {10.1007/s00248-020-01526-5},
	url          = {https://doi.org/10.1007/s00248-020-01526-5},
	abstract     = {Prochlorococcus is the most abundant photosynthetic prokaryote on our planet. The extensive ecological literature on the Prochlorococcus collective (PC) is based on the assumption that it comprises one single genus comprising the species Prochlorococcus marinus , containing itself a collective of ecotypes. Ecologists adopt the distributed genome hypothesis of an open pan-genome to explain the observed genomic diversity and evolution patterns of the ecotypes within PC. Novel genomic data for the PC prompted us to revisit this group, applying the current methods used in genomic taxonomy. As a result, we were able to distinguish the five genera: Prochlorococcus , Eurycolium , Prolificoccus , Thaumococcus and Riococcus . The novel genera have distinct genomic and ecological attributes.}
}
% Software
@article{Price2010,
	title        = {{FastTree 2 - Approximately maximum-likelihood trees for large alignments}},
	author       = {Price, Morgan N. and Dehal, Paramvir S. and Arkin, Adam P.},
	year         = 2010,
	journal      = {PLoS ONE},
	volume       = 5,
	number       = 3,
	doi          = {10.1371/journal.pone.0009490},
	issn         = 19326203,
	abstract     = {Background: We recently described FastTree, a tool for inferring phylogenies for alignments with up to hundreds of thousands of sequences. Here, we describe improvements to FastTree that improve its accuracy without sacrificing scalability. Methodology/Principal Findings: Where FastTree 1 used nearest-neighbor interchanges (NNIs) and the minimum-evolution criterion to improve the tree, FastTree 2 adds minimum-evolution subtree-pruning-regrafting (SPRs) and maximum-likelihood NNIs. FastTree 2 uses heuristics to restrict the search for better trees and estimates a rate of evolution for each site (the "CAT" approximation). Nevertheless, for both simulated and genuine alignments, FastTree 2 is slightly more accurate than a standard implementation of maximum-likelihood NNIs (PhyML 3 with default settings). Although FastTree 2 is not quite as accurate as methods that use maximum-likelihood SPRs, most of the splits that disagree are poorly supported, and for large alignments, FastTree 2 is 100-1,000 times faster. FastTree 2 inferred a topology and likelihood-based local support values for 237,882 distinct 16S ribosomal RNAs on a desktop computer in 22 hours and 5.8 gigabytes of memory. Conclusions/Significance: FastTree 2 allows the inference of maximum-likelihood phylogenies for huge alignments. FastTree 2 is freely available at http://www.microbesonline.org/fasttree. {\textcopyright} 2010 Price et al.}
}
@article{Bray2016,
	title        = {{Near-optimal probabilistic RNA-seq quantification}},
	author       = {Bray, Nicolas L. and Pimentel, Harold and Melsted, P{\'{a}}ll and Pachter, Lior},
	year         = 2016,
	journal      = {Nature Biotechnology},
	volume       = 34,
	number       = 5,
	pages        = {525--527},
	doi          = {10.1038/nbt.3519},
	issn         = 15461696,
	abstract     = {We present kallisto, an RNA-seq quantification program that is two orders of magnitude faster than previous approaches and achieves similar accuracy. Kallisto pseudoaligns reads to a reference, producing a list of transcripts that are compatible with each read while avoiding alignment of individual bases. We use kallisto to analyze 30 million unaligned paired-end RNA-seq reads in {\textless}10 min on a standard laptop computer. This removes a major computational bottleneck in RNA-seq analysis.},
	pmid         = 27043002
}
@misc{Buchfink2014,
	title        = {{Fast and sensitive protein alignment using DIAMOND}},
	author       = {Buchfink, Benjamin and Xie, Chao and Huson, Daniel H.},
	year         = 2014,
	booktitle    = {Nature Methods},
	doi          = {10.1038/nmeth.3176},
	issn         = 15487105,
	abstract     = {The alignment of sequencing reads against a protein reference database is a major computational bottleneck in metagenomics and data-intensive evolutionary projects. Although recent tools offer improved performance over the gold standard BLASTX, they exhibit only a modest speedup or low sensitivity. We introduce DIAMOND, an open-source algorithm based on double indexing that is 20,000 times faster than BLASTX on short reads and has a similar degree of sensitivity.}
}
@article{Camacho2009,
	title        = {{BLAST+: architecture and applications.}},
	author       = {Camacho, Christiam and Coulouris, George and Avagyan, Vahram and Ma, Ning and Papadopoulos, Jason and Bealer, Kevin and Madden, Thomas L},
	year         = 2009,
	month        = dec,
	journal      = {BMC bioinformatics},
	volume       = 10,
	pages        = 421,
	doi          = {10.1186/1471-2105-10-421},
	issn         = {1471-2105},
	url          = {http://www.ncbi.nlm.nih.gov/pubmed/20003500 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2803857},
	abstract     = {BACKGROUND Sequence similarity searching is a very important bioinformatics task. While Basic Local Alignment Search Tool (BLAST) outperforms exact methods through its use of heuristics, the speed of the current BLAST software is suboptimal for very long queries or database sequences. There are also some shortcomings in the user-interface of the current command-line applications. RESULTS We describe features and improvements of rewritten BLAST software and introduce new command-line applications. Long query sequences are broken into chunks for processing, in some cases leading to dramatically shorter run times. For long database sequences, it is possible to retrieve only the relevant parts of the sequence, reducing CPU time and memory usage for searches of short queries against databases of contigs or chromosomes. The program can now retrieve masking information for database sequences from the BLAST databases. A new modular software library can now access subject sequence data from arbitrary data sources. We introduce several new features, including strategy files that allow a user to save and reuse their favorite set of options. The strategy files can be uploaded to and downloaded from the NCBI BLAST web site. CONCLUSION The new BLAST command-line applications, compared to the current BLAST tools, demonstrate substantial speed improvements for long queries as well as chromosome length database sequences. We have also improved the user interface of the command-line applications.},
	pmid         = 20003500
}
@article{Edgar2004,
	title        = {{MUSCLE: Multiple sequence alignment with high accuracy and high throughput}},
	author       = {Edgar, Robert C.},
	year         = 2004,
	journal      = {Nucleic Acids Research},
	volume       = 32,
	number       = 5,
	pages        = {1792--1797},
	doi          = {10.1093/nar/gkh340},
	issn         = {03051048},
	abstract     = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5.com/muscle. {\textcopyright} Oxford University Press 20004; all rights reserved.},
	pmid         = 15034147
}
@article{Hyatt2010,
	title        = {{Prodigal: prokaryotic gene recognition and translation initiation site identification.}},
	author       = {Hyatt, Doug and Chen, Gwo-Liang and Locascio, Philip F and Land, Miriam L and Larimer, Frank W and Hauser, Loren J},
	year         = 2010,
	journal      = {BMC bioinformatics},
	volume       = 11,
	pages        = 119,
	doi          = {10.1186/1471-2105-11-119},
	isbn         = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
	issn         = {1471-2105},
	url          = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2848648{\&}tool=pmcentrez{\&}rendertype=abstract},
	abstract     = {BACKGROUND: The quality of automated gene prediction in microbial organisms has improved steadily over the past decade, but there is still room for improvement. Increasing the number of correct identifications, both of genes and of the translation initiation sites for each gene, and reducing the overall number of false positives, are all desirable goals.$\backslash$n$\backslash$nRESULTS: With our years of experience in manually curating genomes for the Joint Genome Institute, we developed a new gene prediction algorithm called Prodigal (PROkaryotic DYnamic programming Gene-finding ALgorithm). With Prodigal, we focused specifically on the three goals of improved gene structure prediction, improved translation initiation site recognition, and reduced false positives. We compared the results of Prodigal to existing gene-finding methods to demonstrate that it met each of these objectives.$\backslash$n$\backslash$nCONCLUSION: We built a fast, lightweight, open source gene prediction program called Prodigal http://compbio.ornl.gov/prodigal/. Prodigal achieved good results compared to existing methods, and we believe it will be a valuable asset to automated microbial annotation pipelines.},
	keywords     = {Algorithms,Bacterial,Databases,Genetic,Genome,Peptide Chain Initiation,Prokaryotic Cells,Software,Translational,Translational: genetics},
	pmid         = 20211023
}
@article{Katoh2005,
	title        = {{MAFFT version 5: Improvement in accuracy of multiple sequence alignment}},
	author       = {Katoh, Kazutaka and Kuma, Kei Ichi and Toh, Hiroyuki and Miyata, Takashi},
	year         = 2005,
	journal      = {Nucleic Acids Research},
	volume       = 33,
	number       = 2,
	pages        = {511--518},
	doi          = {10.1093/nar/gki198},
	issn         = {03051048},
	abstract     = {The accuracy of multiple sequence alignment proram MAFFT has been improved. The new version (5.3) of MAFFT offers new iterative refinement options, H-INS-i, F-INS-i and G-INS-i, in which pairwise alignment information are incorporated into objective function. These new options of MAFFT showed higher accuracy than currently available methods including TCoffee version 2 and CLUSTAL W in benchmark tests consisting of alignments of {\textgreater}50 sequences. Like the previously available options, the new options of MAFFT can handle hundreds of sequences on a standard desktop computer. We also examined the effect of the number of homologues included in an alignment. For a multiple alignment consisting of ∼8 sequences with low similarity, the accuracy was improved (2-10 percentage points) when the sequences were aligned together with dozens of their close homologues (E-value {\textless} 10-5-10-20) collected from a database. Such improvement was generally observed for most methods, but remarkably large for the new options of MAFFT proposed here. Thus, we made a Ruby script, mafftE.rb, which aligns the input sequences together with their close homologues collected from SwissProt using NCBI-BLAST. {\textcopyright} Oxford University Press 2005; all rights reserved.},
	pmid         = 15661851
}
@article{Menzel2016,
	title        = {{Fast and sensitive taxonomic classification for metagenomics with Kaiju}},
	author       = {Menzel, Peter and Ng, Kim Lee and Krogh, Anders},
	year         = 2016,
	journal      = {Nature Communications},
	volume       = 7,
	doi          = {10.1038/ncomms11257},
	issn         = 20411723,
	abstract     = {Metagenomics emerged as an important field of research not only in microbial ecology but also for human health and disease, and metagenomic studies are performed on increasingly larger scales. While recent taxonomic classification programs achieve high speed by comparing genomic k-mers, they often lack sensitivity for overcoming evolutionary divergence, so that large fractions of the metagenomic reads remain unclassified. Here we present the novel metagenome classifier Kaiju, which finds maximum (in-)exact matches on the protein-level using the Burrows-Wheeler transform. We show in a genome exclusion benchmark that Kaiju classifies reads with higher sensitivity and similar precision compared with current k-mer-based classifiers, especially in genera that are underrepresented in reference databases. We also demonstrate that Kaiju classifies up to 10 times more reads in real metagenomes. Kaiju can process millions of reads per minute and can run on a standard PC. Source code and web server are available at http://kaiju.binf.ku.dk.},
	pmid         = 27071849
}
@article{Seemann2014,
	title        = {{Prokka: Rapid prokaryotic genome annotation}},
	author       = {Seemann, Torsten},
	year         = 2014,
	journal      = {Bioinformatics},
	doi          = {10.1093/bioinformatics/btu153},
	issn         = 14602059,
	abstract     = {The multiplex capability and high yield of current day DNA-sequencing instruments has made bacterial whole genome sequencing a routine affair. The subsequent de novo assembly of reads into contigs has been well addressed. The final step of annotating all relevant genomic features on those contigs can be achieved slowly using existing web- and email-based systems, but these are not applicable for sensitive data or integrating into computational pipelines. Here we introduce Prokka, a command line software tool to fully annotate a draft bacterial genome in about 10 min on a typical desktop computer. It produces standards-compliant output files for further analysis or viewing in genome browsers. {\textcopyright} 2014 The Author 2014.},
	pmid         = 24642063
}
@article{Lakin2017,
	title        = {{MEGARes: An antimicrobial resistance database for high throughput sequencing}},
	author       = {Lakin, Steven M. and Dean, Chris and Noyes, Noelle R. and Dettenwanger, Adam and Ross, Anne Spencer and Doster, Enrique and Rovira, Pablo and Abdo, Zaid and Jones, Kenneth L. and Ruiz, Jaime and Belk, Keith E. and Morley, Paul S. and Boucher, Christina},
	year         = 2017,
	month        = jan,
	journal      = {Nucleic Acids Research},
	publisher    = {Oxford University Press},
	volume       = 45,
	number       = {D1},
	pages        = {D574--D580},
	doi          = {10.1093/nar/gkw1009},
	issn         = 13624962,
	abstract     = {{\textcopyright} The Author(s) 2016. Antimicrobial resistance has become an imminent concern for public health. As methods for detection and characterization of antimicrobial resistance move from targeted culture and polymerase chain reaction to high throughput metagenomics, appropriate resources for the analysis of large-scale data are required. Currently, antimicrobial resistance databases are tailored to smaller-scale, functional profiling of genes using highly descriptive annotations. Such characteristics do not facilitate the analysis of large-scale, ecological sequence datasets such as those produced with the use of metagenomics for surveillance. In order to overcome these limitations, we present MEGARes (https://megares.meglab.org), a hand-curated antimicrobial resistance database and annotation structure that provides a foundation for the development of high throughput acyclical classifiers and hierarchical statistical analysis of big data. MEGARes can be browsed as a stand-alone resource through the website or can be easily integrated into sequence analysis pipelines through download. Also via the website, we provide documentation for AmrPlusPlus, a user-friendly Galaxy pipeline for the analysis of high throughput sequencing data that is pre-packaged for use with the MEGARes database.}
}
@article{OLeary2016,
	title        = {{Reference sequence (RefSeq) database at NCBI: Current status, taxonomic expansion, and functional annotation}},
	author       = {O'Leary, Nuala A. and Wright, Mathew W. and Brister, J. Rodney and Ciufo, Stacy and Haddad, Diana and McVeigh, Rich and Rajput, Bhanu and Robbertse, Barbara and Smith-White, Brian and Ako-Adjei, Danso and Astashyn, Alexander and Badretdin, Azat and Bao, Yiming and Blinkova, Olga and Brover, Vyacheslav and Chetvernin, Vyacheslav and Choi, Jinna and Cox, Eric and Ermolaeva, Olga and Farrell, Catherine M. and Goldfarb, Tamara and Gupta, Tripti and Haft, Daniel and Hatcher, Eneida and Hlavina, Wratko and Joardar, Vinita S. and Kodali, Vamsi K. and Li, Wenjun and Maglott, Donna and Masterson, Patrick and McGarvey, Kelly M. and Murphy, Michael R. and O'Neill, Kathleen and Pujar, Shashikant and Rangwala, Sanjida H. and Rausch, Daniel and Riddick, Lillian D. and Schoch, Conrad and Shkeda, Andrei and Storz, Susan S. and Sun, Hanzhen and Thibaud-Nissen, Francoise and Tolstoy, Igor and Tully, Raymond E. and Vatsan, Anjana R. and Wallin, Craig and Webb, David and Wu, Wendy and Landrum, Melissa J. and Kimchi, Avi and Tatusova, Tatiana and DiCuccio, Michael and Kitts, Paul and Murphy, Terence D. and Pruitt, Kim D.},
	year         = 2016,
	journal      = {Nucleic Acids Research},
	doi          = {10.1093/nar/gkv1189},
	issn         = 13624962,
	pmid         = 26553804
}
@article{Kitts2015,
	title        = {{Assembly: a resource for assembled genomes at NCBI}},
	author       = {Kitts, Paul A and Church, Deanna M and Thibaud-Nissen, Fran{\c{c}}oise and Choi, Jinna and Hem, Vichet and Sapojnikov, Victor and Smith, Robert G and Tatusova, Tatiana and Xiang, Charlie and Zherikov, Andrey and DiCuccio, Michael and Murphy, Terence D and Pruitt, Kim D and Kimchi, Avi},
	year         = 2015,
	journal      = {Nucleic Acids Research},
	volume       = 44,
	number       = {D1},
	pages        = {D73--D80},
	doi          = {10.1093/nar/gkv1226},
	issn         = {0305-1048},
	url          = {https://doi.org/10.1093/nar/gkv1226}
}
@article{Jain2018,
	title        = {{High throughput ANI analysis of 90K prokaryotic genomes reveals clear species boundaries}},
	author       = {Jain, Chirag and Rodriguez-R, Luis M and Phillippy, Adam M and Konstantinidis, Konstantinos T and Aluru, Srinivas},
	year         = 2018,
	month        = nov,
	journal      = {Nature communications},
	publisher    = {Nature Publishing Group UK},
	volume       = 9,
	number       = 1,
	pages        = 5114,
	doi          = {10.1038/s41467-018-07641-9},
	issn         = {2041-1723},
	url          = {https://www.ncbi.nlm.nih.gov/pubmed/30504855 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6269478/},
	abstract     = {A fundamental question in microbiology is whether there is continuum of genetic diversity among genomes, or clear species boundaries prevail instead. Whole-genome similarity metrics such as Average Nucleotide Identity (ANI) help address this question by facilitating high resolution taxonomic analysis of thousands of genomes from diverse phylogenetic lineages. To scale to available genomes and beyond, we present FastANI, a new method to estimate ANI using alignment-free approximate sequence mapping. FastANI is accurate for both finished and draft genomes, and is up to three orders of magnitude faster compared to alignment-based approaches. We leverage FastANI to compute pairwise ANI values among all prokaryotic genomes available in the NCBI database. Our results reveal clear genetic discontinuity, with 99.8{\%} of the total 8 billion genome pairs analyzed conforming to {\textgreater}95{\%} intra-species and {\textless}83{\%} inter-species ANI values. This discontinuity is manifested with or without the most frequently sequenced species, and is robust to historic additions in the genome databases.},
	keywords     = {Databases, Factual,Genetic Variation/genetics,Genome, Bacterial/genetics,Phylogeny,Prokaryotic Cells/*metabolism,Sequence Analysis, DNA},
	language     = {eng}
}
@article{Gevers2005,
	title        = {{Reevaluating prokaryotic species}},
	author       = {Gevers, Dirk and Cohan, Frederick M and Lawrence, Jeffrey G and Spratt, Brian G and Coenye, Tom and Feil, E J and Stackebrandt, Erko and Peer, Yves Van De and Vandamme, Peter and Thompson, Fabiano L and Swings, Jean},
	year         = 2005,
	journal      = {Nature Reviews Microbiology},
	volume       = 3,
	number       = {September},
	pages        = {733--739}
}
@article{Rodriguez2014,
	title        = {{Bypassing Cultivation To Identify Bacterial Species}},
	author       = {Rodriguez-R, Luis M and Konstantinidis, Konstantinos T},
	year         = 2014,
	journal      = {Microbe},
	volume       = 9,
	number       = 3,
	pages        = {111--117},
	url          = {http://enve-omics.gatech.edu/sites/default/files/2014-Rodriguez{\_}R-Konstantinidis{\_}Microbe{\_}Magazine.pdf}
}
@article{voorhees1986,
	title        = {{Implementing agglomerative hierarchic clustering algorithms for use in document retrieval}},
	author       = {Voorhees, Ellen M},
	year         = 1986,
	journal      = {Information Processing {\&} Management},
	publisher    = {Elsevier},
	volume       = 22,
	number       = 6,
	pages        = {465--476}
}
@article{Sayers2020,
	title        = {{GenBank}},
	author       = {Sayers, Eric W. and Cavanaugh, Mark and Clark, Karen and Ostell, James and Pruitt, Kim D. and Karsch-Mizrachi, Ilene},
	year         = 2020,
	journal      = {Nucleic Acids Research},
	volume       = 48,
	number       = {D1},
	pages        = {D84--D86},
	doi          = {10.1093/nar/gkz956},
	issn         = 13624962,
	abstract     = {GenBank{\textregistered} (www.ncbi.nlm.nih.gov/genbank/) is a comprehensive, public database that contains over 6.25 trillion base pairs from over 1.6 billion nucleotide sequences for 450 000 formally described species. Daily data exchange with the European Nucleotide Archive (ENA) and the DNA Data Bank of Japan (DDBJ) ensures worldwide coverage. Recent updates include a new version of Genome Workbench that supports GenBank submissions, new submission wizards for viral genomes, enhancements to BankIt and improved handling of taxonomy for sequences from pathogens.},
	pmid         = 31665464
}
@article{hugenholtz2016genome,
	title        = {{Genome-based microbial taxonomy coming of age}},
	author       = {Hugenholtz, Philip and Skarshewski, Adam and Parks, Donovan H},
	year         = 2016,
	journal      = {Cold Spring Harbor Perspectives in Biology},
	publisher    = {Cold Spring Harbor Lab},
	volume       = 8,
	number       = 6,
	pages        = {a018085}
}
@article{Konstantinidis2005,
	title        = {{Towards a genome-based taxonomy for prokaryotes}},
	author       = {Konstantinidis, Konstantinos T. and Tiedje, James M.},
	year         = 2005,
	journal      = {Journal of Bacteriology},
	volume       = 187,
	number       = 18,
	pages        = {6258--6264},
	doi          = {10.1128/JB.187.18.6258-6264.2005},
	issn         = {00219193},
	abstract     = {The ranks higher than the species in the prokaryotic taxonomy are primarily designated based on phylogenetic analysis of the 16S rRNA gene sequences, but no definite standards exist for the absolute relatedness (measured by 16S rRNA or other means) between the ranks. Accordingly, it remains unknown how comparable the ranks are between different organisms. To gain insights into this question, we studied the relationship between shared gene content and genetic relatedness for 175 fully sequenced strains, using as a robust measure of relatedness the average amino acid identity (AAI) of the shared genes. Our results reveal that adjacent ranks (e.g., phylum versus class) frequently show extensive overlap in terms of genetic and gene content relatedness of the grouped organisms, and hence, the current system is of limited predictive power in this respect. The overlap between nonadjacent ranks (e.g., phylum versus family) is generally limited and attributable to clear inconsistencies of the taxonomy. In addition to providing means for standardizing taxonomy, our AAI-based approach provides a means to evaluate the robustness of alternative genetic markers for phylogenetic purposes. For instance, the 23S rRNA gene was found to be as good a marker as the 16S rRNA gene, while several of the widely distributed protein-coding genes, such as the RNA polymerase and gyrase subunits, show a strong phylogenetic signal, albeit less strong than the rRNA genes (0.78 {\textgreater} R2 {\textgreater} 0.69 for the protein-coding genes versus R2 = 0.84 for the rRNA genes). The AAI approach outlined here could contribute significantly to a genome-based taxonomy for all microbial organisms.}
}
@article{Flombaum2013,
	title        = {{Present and future global distributions of the marine Cyanobacteria Prochlorococcus and Synechococcus}},
	author       = {Flombaum, P. and Gallegos, J. L. and Gordillo, R. A. and Rincon, J. and Zabala, L. L. and Jiao, N. and Karl, D. M. and Li, W. K. W. and Lomas, M. W. and Veneziano, D. and Vera, C. S. and Vrugt, J. A. and Martiny, A. C.},
	year         = 2013,
	journal      = {Proceedings of the National Academy of Sciences},
	volume       = 110,
	number       = 24,
	pages        = {9824--9829},
	doi          = {10.1073/pnas.1307701110},
	issn         = {0027-8424},
	url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.1307701110},
	abstract     = {The Cyanobacteria Prochlorococcus and Synechococcus account for a substantial fraction of marine primary production. Here, we pres- ent quantitative niche models for these lineages that assess present and future global abundances and distributions. These niche models are the result of neural network, nonparametric, and parametric analyses, and they rely on {\textgreater} 35,000 discrete observations from all major ocean regions. The models assess cell abundance based on temperature and photosynthetically active radiation, but the indi- vidual responses to these environmental variables differ for each lineage. The models estimate global biogeographic patterns and seasonal variability of cell abundance, with maxima in the warm oligotrophic gyres of the Indian and the western Paci fi cOceans and minima at higher latitudes. The annual mean global abundances of Prochlorococcus and Synechococcus are 2.9 ± 0.1 × 10 27 and 7.0 ± 0.3 × 10 26 cells, respectively. Using projections of sea surface tem- perature as a result of increased concentration of greenhouse gases at the end of the 21st century, our niche models projected increases in cell numbers of 29{\%} and 14{\%} for Prochlorococcus and Synecho- coccus , respectively. The changes are geographically uneven but include an increase in area. Thus, our global niche models suggest that oceanic microbial communities will experience complex changes as a result of projected future climate conditions. Because of the high abundances and contributions to primary production of Prochloro- coccus and Synechococcus , these changes may have large impacts on ocean ecosystems and biogeochemical cycles.}
}
@article{Abed2009,
	title        = {{Applications of cyanobacteria in biotechnology}},
	author       = {Abed, R.M.M. and Dobretsov, S. and Sudesh, K.},
	year         = 2009,
	month        = jan,
	journal      = {Journal of Applied Microbiology},
	publisher    = {John Wiley {\&} Sons, Ltd (10.1111)},
	volume       = 106,
	number       = 1,
	pages        = {1--12},
	doi          = {10.1111/j.1365-2672.2008.03918.x},
	issn         = 13645072,
	url          = {http://doi.wiley.com/10.1111/j.1365-2672.2008.03918.x},
	keywords     = {bioactive compounds,biofertilizers,bioplastics,biotechnology,cyanobacteria}
}
@article{Biller2015,
	title        = {{Prochlorococcus: The structure and function of collective diversity}},
	author       = {Biller, Steven J. and Berube, Paul M. and Lindell, Debbie and Chisholm, Sallie W.},
	year         = 2015,
	journal      = {Nature Reviews Microbiology},
	publisher    = {Nature Publishing Group},
	volume       = 13,
	number       = 1,
	pages        = {13--27},
	doi          = {10.1038/nrmicro3378},
	issn         = 17401534,
	url          = {http://dx.doi.org/10.1038/nrmicro3378},
	abstract     = {The marine cyanobacterium Prochlorococcus is the smallest and most abundant photosynthetic organism on Earth. In this Review, we summarize our understanding of the diversity of this remarkable phototroph and describe its role in ocean ecosystems. We discuss the importance of interactions of Prochlorococcus with the physical environment, with phages and with heterotrophs in shaping the ecology and evolution of this group. In light of recent studies, we have come to view Prochlorococcus as a 'federation' of diverse cells that sustains its broad distribution, stability and abundance in the oceans via extensive genomic and phenotypic diversity. Thus, it is proving to be a useful model system for elucidating the forces that shape microbial populations and ecosystems.}
}
@article{Coleman2007,
	title        = {{Code and context: Prochlorococcus as a model for cross-scale biology}},
	author       = {Coleman, Maureen L. and Chisholm, Sallie W.},
	year         = 2007,
	journal      = {Trends in Microbiology},
	volume       = 15,
	number       = 9,
	pages        = {398--407},
	doi          = {10.1016/j.tim.2007.07.001},
	issn         = {0966842X},
	abstract     = {Prochlorococcus is a simple cyanobacterium that is abundant throughout large regions of the oceans, and has become a useful model for studying the nature and regulation of biological diversity across all scales of complexity. Recent work has revealed that environmental factors such as light, nutrients and predation influence diversity in different ways, changing our image of the structure and dynamics of the global Prochlorococcus population. Advances in metagenomics, transcription profiling and global ecosystem modeling promise to deliver an even greater understanding of this system and further demonstrate the power of cross-scale systems biology. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.}
}
@article{Walter2017,
	title        = {{Ecogenomics and taxonomy of Cyanobacteria phylum}},
	author       = {Walter, Juline M. and Coutinho, Felipe H. and Dutilh, Bas E. and Swings, Jean and Thompson, Fabiano L. and Thompson, Cristiane C.},
	year         = 2017,
	journal      = {Frontiers in Microbiology},
	volume       = 8,
	number       = {NOV},
	doi          = {10.3389/fmicb.2017.02132},
	issn         = {1664302X},
	abstract     = {Cyanobacteria are major contributors to global biogeochemical cycles. The genetic diversity among Cyanobacteria enables them to thrive across many habitats, although only a few studies have analysed the association of phylogenomic clades to specific environmental niches. In this study, we adopted an ecogenomics strategy with the aim to delineate ecological niche preferences of Cyanobacteria and integrate them to the genomic taxonomy of these bacteria. First, an appropriate phylogenomic framework was established using a set of genomic taxonomy signatures (including a tree based on conserved gene sequences, genome-to-genome distance, and average amino acid identity) to analyse ninety-nine publicly available cyanobacterial genomes. Next, the relative abundances of these genomes were determined throughout diverse global marine and freshwater ecosystems, using metagenomic data sets. The whole-genome-based taxonomy of the ninety-nine genomes allowed us to identify 57 (of which 28 are new genera) and 87 (of which 32 are new species) different cyanobacterial genera and species, respectively. The ecogenomic analysis allowed the distinction of three major ecological groups of Cyanobacteria (named as i. Low Temperature; ii. Low Temperature Copiotroph; and iii. High Temperature Oligotroph) that were coherently linked to the genomic taxonomy. This work establishes a new taxonomic framework for Cyanobacteria in the light of genomic taxonomy and ecogenomic approaches.},
	keywords     = {Charting biodiversity,Ecological niches,Genome-based microbial taxonomy,High-throughput sequencing technology,Metagenome,Microbial ecology}
}
@article{Coutinho2016a,
	title        = {{Comparative genomics of Synechococcus and proposal of the new genus Parasynechococcus}},
	author       = {Coutinho, Felipe and Tschoeke, Diogo Antonio and Thompson, Fabiano and Thompson, Cristiane},
	year         = 2016,
	journal      = {PeerJ},
	volume       = 4,
	pages        = {e1522},
	doi          = {10.7717/peerj.1522},
	abstract     = {Synechococcus is among the most important contributors to global primary productivity. The genomes of several strains of this taxon have been previously sequenced in an effort to understand the physiology and ecology of these highly diverse microorganisms. Here we present a comparative study of Synechococcus genomes. For that end, we developed GenTaxo, a program written in Perl to perform genomic taxonomy based on average nucleotide identity, average amino acid identity and dinucleotide signatures, which revealed that the analyzed strains are drastically distinct regarding their genomic content. Phylogenomic reconstruction indicated a division of Synechococcus in two clades (i.e. Synechococcus and the new genus Parasynechococcus), corroborating evidences that this is in fact a polyphyletic group. By clustering protein encoding genes into homologue groups we were able to trace the Pangenome and core genome of both marine and freshwater Synechococcus and determine the genotypic traits that differentiate these lineages.}
}
@article{Thompson2013b,
	title        = {{Genomic Taxonomy of the Genus Prochlorococcus}},
	author       = {Thompson, Cristiane C. and Silva, Genivaldo G.Z. and Vieira, Nayra M. and Edwards, Robert and Vicente, Ana Carolina P. and Thompson, Fabiano L.},
	year         = 2013,
	journal      = {Microbial Ecology},
	volume       = 66,
	number       = 4,
	pages        = {752--762},
	doi          = {10.1007/s00248-013-0270-8},
	issn         = {00953628},
	abstract     = {The genus Prochlorococcus is globally abundant and dominates the total phytoplankton biomass and production in the oligotrophic ocean. The single species, Prochlorococcus marinus, comprises six named ecotypes. Our aim was to analyze the taxonomic structure of the genus Prochlorococcus. We analyzed the complete genomes of 13 cultured P. marinus type and reference strains by means of several genomic taxonomy tools (i.e., multilocus sequence analysis, amino acid identity, Karlin genomic signature, and genome to genome distance). In addition, we estimated the diversity of Prochlorococcus species in over 100 marine metagenomes from all the major oceanic provinces. According to our careful taxonomic analysis, the 13 strains corresponded, in fact, to ten different Prochlorococcus species. This analysis establishes a new taxonomic framework for the genus Prochlorococcus. Further, the analysis of the metagenomic data suggests that, in total, there may only be 35 Prochlorococcus species in the world's oceans. We propose that the dearth of species observed in this study is driven by high selective pressures that limit diversification in the global ocean.}
}
@article{Coutinho2016b,
	title        = {{Proposal of fifteen new species of Parasynechococcus based on genomic, physiological and ecological features}},
	author       = {Coutinho, F. H. and Dutilh, B. E. and Thompson, C. C. and Thompson, F. L.},
	year         = 2016,
	journal      = {Archives of Microbiology},
	publisher    = {Springer Berlin Heidelberg},
	volume       = 198,
	number       = 10,
	pages        = {973--986},
	doi          = {10.1007/s00203-016-1256-y},
	issn         = {1432072X},
	abstract     = {{\textcopyright} 2016 Springer-Verlag Berlin HeidelbergMembers of the recently proposed genus Parasynechococcus (Cyanobacteria) are extremely abundant throughout the global ocean and contribute significantly to global primary productivity. However, the taxonomy of these organisms remains poorly characterized. The aim of this study was to propose a new taxonomic framework for Parasynechococcus based on a genomic taxonomy approach that incorporates genomic, physiological and ecological data. Through in silico DNA–DNA hybridization, average amino acid identity, dinucleotide signatures and phylogenetic reconstruction, a total of 15 species of Parasynechococcus could be delineated. Each species was then described on the basis of their gene content, light and nutrient utilization strategies, geographical distribution patterns throughout the oceans and response to environmental parameters.},
	keywords     = {Cyanobacteria,Genomic taxonomy,Pan-genome,Parasynechococcus}
}
@article{Thompson2013a,
	title        = {{Microbial genomic taxonomy}},
	author       = {Thompson, Cristiane C. and Chimetto, Luciane and Edwards, Robert A. and Swings, Jean and Stackebrandt, Erko and Thompson, Fabiano L.},
	year         = 2013,
	journal      = {BMC Genomics},
	volume       = 14,
	number       = 1,
	doi          = {10.1186/1471-2164-14-913},
	issn         = 14712164,
	abstract     = {A need for a genomic species definition is emerging from several independent studies worldwide. In this commentary paper, we discuss recent studies on the genomic taxonomy of diverse microbial groups and a unified species definition based on genomics. Accordingly, strains from the same microbial species share {\textgreater}95{\%} Average Amino Acid Identity (AAI) and Average Nucleotide Identity (ANI), {\textgreater}95{\%} identity based on multiple alignment genes, {\textless}10 in Karlin genomic signature, and {\textgreater} 70{\%} in silico Genome-to-Genome Hybridization similarity (GGDH). Species of the same genus will form monophyletic groups on the basis of 16S rRNA gene sequences, Multilocus Sequence Analysis (MLSA) and supertree analysis. In addition to the established requirements for species descriptions, we propose that new taxa descriptions should also include at least a draft genome sequence of the type strain in order to obtain a clear outlook on the genomic landscape of the novel microbe. The application of the new genomic species definition put forward here will allow researchers to use genome sequences to define simultaneously coherent phenotypic and genomic groups.},
	keywords     = {Evolution,Genomics,Microbes,Taxonomy}
}
@book{abadi2012theory,
	title        = {A theory of objects},
	author       = {Abadi, Martin and Cardelli, Luca},
	year         = 2012,
	publisher    = {Springer Science \& Business Media}
}
@techreport{ProvOverview2013,
	title        = {{PROV-Overview: An Overview of the PROV Family of Documents}},
	author       = {{W3C Provenance Working Group}},
	year         = 2013,
	url          = {https://www.w3.org/TR/prov-overview/}
}
@article{nowogrodzki2019,
	title        = {How to support open-source software and stay sane},
	author       = {Nowogrodzki, Anna},
	year         = 2019,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 571,
	number       = 7763,
	pages        = {133--135}
}
@article{TenHoopen2017,
	title        = {{The metagenomic data life-cycle: standards and best practices.}},
	author       = {{Ten Hoopen}, Petra and Finn, Robert D and Bongo, Lars Ailo and Corre, Erwan and Fosso, Bruno and Meyer, Folker and Mitchell, Alex and Pelletier, Eric and Pesole, Graziano and Santamaria, Monica and Willassen, Nils Peder and Cochrane, Guy},
	year         = 2017,
	month        = aug,
	journal      = {GigaScience},
	volume       = 6,
	number       = 8,
	pages        = {1--11},
	doi          = {10.1093/gigascience/gix047},
	issn         = {2047-217X (Electronic)},
	abstract     = {Metagenomics data analyses from independent studies can only be compared if the  analysis workflows are described in a harmonized way. In this overview, we have mapped the landscape of data standards available for the description of essential steps in metagenomics: (i) material sampling, (ii) material sequencing, (iii) data analysis, and (iv) data archiving and publishing. Taking examples from marine research, we summarize essential variables used to describe material sampling processes and sequencing procedures in a metagenomics experiment. These aspects of metagenomics dataset generation have been to some extent addressed by the scientific community, but greater awareness and adoption is still needed. We emphasize the lack of standards relating to reporting how metagenomics datasets are analysed and how the metagenomics data analysis outputs should be archived and published. We propose best practice as a foundation for a community standard to enable reproducibility and better sharing of metagenomics datasets, leading ultimately to greater metagenomics data reuse and repurposing.},
	keywords     = {Computational Biology,Data Mining,Databases, Genetic,Metagenome,Metagenomics,Sequence Analysis,Workflow,methods,standards},
	language     = {eng},
	pmid         = 28637310
}
@article{Attwood2017,
	title        = {{A global perspective on evolving bioinformatics and data science training needs}},
	author       = {Attwood, Teresa K. and Blackford, Sarah and Brazas, Michelle D. and Davies, Angela and Schneider, Maria Victoria},
	year         = 2017,
	journal      = {Briefings in Bioinformatics},
	volume       = 20,
	number       = 2,
	pages        = {398--404},
	doi          = {10.1093/bib/bbx100},
	issn         = 14774054,
	abstract     = {Bioinformatics is now intrinsic to life science research, but the past decade has witnessed a continuing deficiency in this essential expertise. Basic data stewardship is still taught relatively rarely in life science education programmes, creating a chasm between theory and practice, and fuelling demand for bioinformatics training across all educational levels and career roles. Concerned by this, surveys have been conducted in recent years to monitor bioinformatics and computational training needs worldwide. This article briefly reviews the principal findings of a number of these studies. We see that there is still a strong appetite for short courses to improve expertise and confidence in data analysis and interpretation; strikingly, however, the most urgent appeal is for bioinformatics to be woven into the fabric of life science degree programmes. Satisfying the relentless training needs of current and future generations of life scientists will require a concerted response from stakeholders across the globe, who need to deliver sustainable solutions capable of both transforming education curricula and cultivating a new cadre of trainer scientists.},
	keywords     = {bioinformatics training,computational and statistical competency,data science,skills gap,training survey,training trainers}
}
@article{Buneman2001,
	title        = {{Why and Where: A Characterization of Data Provenance}},
	author       = {Buneman, Peter and Khanna, Sanjeev and Tan, Wang-Chiew and Chiew, Wang -},
	year         = 2001,
	journal      = {Lecture Notes in Computer Science},
	volume       = 1973,
	number       = {January},
	pages        = {316--330},
	url          = {https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://www.google.com/{\&}httpsredir=1{\&}article=1209{\&}context=cis{\_}papers},
	abstract     = {With the proliferation of database views and curated databases, the issue of data provenance-where a piece of data came from and the process by which it arrived in the database-is becoming increasingly important, especially in scientific databases where understanding provenance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. We adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between "why" provenance (refers to the source data that had some influence on the existence of the data) and "where" provenance (refers to the location(s) in the source databases from which the data was extracted). Abstract. With the proliferation of database views and curated databases, the issue of data provenance where a piece of data came from and the process by which it arrived in the database e is becoming increasingly important, especially in scientiic databases where understanding prove-nance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. W e adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between nwhy" provenance refers to the source data that had some innuence on the existence of the data and dwhere" provenance refers to the locations in the source databases from which the data was extracted.}
}
@book{Groth2013,
	title        = {{An Introduction to PROV}},
	author       = {Groth, Paul and Moreau, Luc},
	year         = 2013,
	publisher    = {Morgan {\&} Claypool},
	pages        = 131,
	doi          = {10.2200/S00528ED1V01Y201308WEB007},
	isbn         = 9781627052214,
	editor       = {Hendler, James and Ding, Ying}
}
@article{Woese2004,
	title        = {{A new biology for a new century.}},
	author       = {Woese, Carl R},
	year         = 2004,
	month        = jun,
	journal      = {Microbiology and molecular biology reviews : MMBR},
	volume       = 68,
	number       = 2,
	pages        = {173--186},
	doi          = {10.1128/MMBR.68.2.173-186.2004},
	issn         = {1092-2172 (Print)},
	abstract     = {Biology today is at a crossroads. The molecular paradigm, which so successfully guided the discipline throughout most of the 20th century, is no longer a reliable guide. Its vision of biology now realized, the molecular paradigm has run its course. Biology, therefore, has a choice to make, between the comfortable path of continuing to follow molecular biology's lead or the more invigorating one of seeking a new and inspiring vision of the living world, one that addresses the major problems in biology that 20th century biology, molecular biology, could not handle and, so, avoided. The former course, though highly productive, is certain to turn biology into an engineering discipline. The latter holds the promise of making biology an even more fundamental science, one that, along with physics, probes and defines the nature of reality. This is a choice between a biology that solely does society's bidding and a biology that is society's teacher.},
	keywords     = {21st Century,Biology,History,Science,history,trends},
	language     = {eng},
	pmid         = 15187180
}
@book{Lesk2019,
	title        = {Bioinformatics},
	author       = {Lesk, Arthur},
	year         = 2019,
	publisher    = {Encyclop{\ae}dia Britannica}
}
@article{Altschul1990,
	title        = {{Basic local alignment search tool}},
	author       = {Altschul, Stephen F. and Gish, Warren and Miller, Webb and Myers, Eugene W. and Lipman, David J.},
	year         = 1990,
	journal      = {Journal of Molecular Biology},
	volume       = 215,
	number       = 3,
	pages        = {403--410},
	doi          = {10.1016/S0022-2836(05)80360-2},
	issn         = {00222836},
	abstract     = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straight-forward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity. {\textcopyright} 1990, Academic Press Limited. All rights reserved.},
	pmid         = 2231712
}
@article{larkin2007clustal,
	title        = {{Clustal W and Clustal X version 2.0}},
	author       = {Larkin, Mark A and Blackshields, Gordon and Brown, Nigel P and Chenna, R and McGettigan, Paul A and McWilliam, Hamish and Valentin, Franck and Wallace, Iain M and Wilm, Andreas and Lopez, Rodrigo and Others},
	year         = 2007,
	journal      = {bioinformatics},
	publisher    = {Oxford University Press},
	volume       = 23,
	number       = 21,
	pages        = {2947--2948}
}
@article{Needleman1970,
	title        = {{A general method applicable to the search for similarities in the amino acid sequence of two proteins}},
	author       = {Needleman, Saul B. and Wunsch, Christian D.},
	year         = 1970,
	month        = mar,
	journal      = {Journal of Molecular Biology},
	volume       = 48,
	number       = 3,
	pages        = {443--453},
	doi          = {10.1016/0022-2836(70)90057-4},
	issn         = {00222836},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/0022283670900574}
}
@article{Smith1981,
	title        = {{Identification of common molecular subsequences}},
	author       = {Smith, T. F. and Waterman, M. S.},
	year         = 1981,
	journal      = {Journal of Molecular Biology},
	volume       = 147,
	number       = 1,
	pages        = {195--197},
	doi          = {10.1016/0022-2836(81)90087-5},
	issn         = {00222836},
	pmid         = 7265238
}
@book{Durbin1998,
	title        = {{Biological Sequence Analysis}},
	author       = {Durbin, Richard and Eddy, Sean R. and Krogh, Anders and Mitchison, Graeme},
	year         = 1998,
	booktitle    = {Biological Sequence Analysis},
	doi          = {10.1017/cbo9780511790492},
	abstract     = {The face of biology has been changed by the emergence of modem molecular genetics. Among the most exciting advances are large-scale DNA sequencing efforts such as the Human Genome Project which are producing an immense amount of data. The need to understand the data is becoming ever more pressing. Demands for sophisticated analyses of biological sequences are driving forward the newly-created and explosively expanding research area of computational molecular biology, or bioinformatics. Many of the most powerful sequence analysis methods are now based on principles of probabilistic modelling. Examples of such methods include the use of probabilistically derived score matrices to determine the significance of sequence alignments, the use of hidden Markov models as the basis for profile searches to identify distant members of sequence families, and the inference of phylogenetic trees using maximum likelihood approaches. This book provides the first unified, up-to-date, and tutorial-level overview of sequence analysis methods, with particular emphasis on probabilistic modelling. Pairwise alignment, hidden Markov models, multiple alignment, profile searches, RNA secondary structure analysis, and phylogenetic inference are treated at length. Written by an interdisciplinary team of authors, the book is accessible to molecular biologists, computer scientists and mathematicians with n o formal knowledge of each others' fields. It presents the state-of-the-art in this important, new and rapidly developing discipline.}
}
@article{McMurdie2014,
	title        = {{Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible}},
	author       = {McMurdie, Paul J. and Holmes, Susan},
	year         = 2014,
	journal      = {PLoS Computational Biology},
	volume       = 10,
	number       = 4,
	doi          = {10.1371/journal.pcbi.1003531},
	isbn         = {1553-7358 (Electronic) 1553-734X (Linking)},
	issn         = 15537358,
	abstract     = {The interpretation of count data originating from the current generation of DNA sequencing platforms requires special attention. In particular, the per-sample library sizes often vary by orders of magnitude from the same sequencing run, and the counts are overdispersed relative to a simple Poisson model These challenges can be addressed using an appropriate mixture model that simultaneously accounts for library size differences and biological variability. This approach is already well-characterized and implemented for RNA-Seq data in R packages such as edgeR and DESeq. We use statistical theory, extensive simulations, and empirical data to show that variance stabilizing normalization using a mixture model like the negative binomial is appropriate for microbiome count data. In simulations detecting differential abundance, normalization procedures based on a Gamma-Poisson mixture model provided systematic improvement in performance over crude proportions or rarefied counts -- both of which led to a high rate of false positives. In simulations evaluating clustering accuracy, we found that the rarefying procedure discarded samples that were nevertheless accurately clustered by alternative methods, and that the choice of minimum library size threshold was critical in some settings, but with an optimum that is unknown in practice. Techniques that use variance stabilizing transformations by modeling microbiome count data with a mixture distribution, such as those implemented in edgeR and DESeq, substantially improved upon techniques that attempt to normalize by rarefying or crude proportions. Based on these results and well-established statistical theory, we advocate that investigators avoid rarefying altogether. We have provided microbiome-specific extensions to these tools in the R package, phyloseq.},
	archiveprefix = {arXiv},
	arxivid      = {1310.0424},
	eprint       = {1310.0424},
	pmid         = 24699258
}
@article{Reiter2021,
	title        = {{Streamlining data-intensive biology with workflow systems}},
	author       = {Reiter, Taylor and Brooks, Phillip T and Irber, Luiz and Joslin, Shannon E K and Reid, Charles M and Scott, Camille and Brown, C Titus and Pierce-Ward, N Tessa},
	year         = 2021,
	month        = jan,
	journal      = {GigaScience},
	volume       = 10,
	number       = 1,
	doi          = {10.1093/gigascience/giaa140},
	issn         = {2047-217X},
	url          = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giaa140/6092773},
	abstract     = {As the scale of biological data generation has increased, the bottleneck of research has shifted from data generation to analysis. Researchers commonly need to build computational workflows that include multiple analytic tools and require incremental development as experimental insights demand tool and parameter modifications. These workflows can produce hundreds to thousands of intermediate files and results that must be integrated for biological insight. Data-centric workflow systems that internally manage computational resources, software, and conditional execution of analysis steps are reshaping the landscape of biological data analysis and empowering researchers to conduct reproducible analyses at scale. Adoption of these tools can facilitate and expedite robust data analysis, but knowledge of these techniques is still lacking. Here, we provide a series of strategies for leveraging workflow systems with structured project, data, and resource management to streamline large-scale biological analysis. We present these practices in the context of high-throughput sequencing data analysis, but the principles are broadly applicable to biologists working beyond this field.}
}
@book{Albert2021,
	title        = {{The BioStars Handbook 2nd Edition}},
	author       = {Albert, Istv{\'{a}}n},
	year         = 2021,
	isbn         = {978-0-578-80435-4}
}
@misc{Baker2016,
	title        = {1,500 scientists lift the lid on reproducibility.},
	author       = {Baker, Monya},
	year         = 2016,
	month        = may,
	booktitle    = {Nature},
	volume       = 533,
	number       = 7604,
	pages        = {452--454},
	doi          = {10.1038/533452a},
	issn         = {1476-4687 (Electronic)},
	keywords     = {Attitude,Data Interpretation, Statistical,Mentors,Periodicals as Topic,Publishing,Reproducibility of Results,Research,Research Design,Research Personnel,Research Support as Topic,Surveys and Questionnaires,economics,methods,psychology,standards,statistics {\&} numerical data,trends},
	language     = {eng},
	pmid         = 27225100
}
@inproceedings{Ocana2015,
	title        = {{Exploiting the Parallel Execution of Homology Workflow Alternatives in HPC Compute Clouds}},
	author       = {Oca{\~{n}}a, Kary A C S and de Oliveira, Daniel and Silva, V{\'{i}}tor and Benza, Silvia and Mattoso, Marta},
	year         = 2015,
	booktitle    = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	pages        = {336--350},
	doi          = {10.1007/978-3-319-22885-3_29},
	url          = {http://link.springer.com/10.1007/978-3-319-22885-3{\_}29},
	keywords     = {consistent with the outcome,conventional wet laboratory experiments,data,homology modeling,management,of,provenance,scicumulus,simulation prove to be,workflow}
}
@article{Ocana2012a,
	title        = {{Discovering drug targets for neglected diseases using a pharmacophylogenomic cloud workflow}},
	author       = {Oca{\~{n}}a, Kary A.C.S. and {De Oliveira}, Daniel and Dias, Jonas and Ogasawara, Eduardo and Mattoso, Marta},
	year         = 2012,
	journal      = {2012 IEEE 8th International Conference on E-Science, e-Science 2012},
	doi          = {10.1109/eScience.2012.6404431},
	isbn         = 9781467344678,
	abstract     = {Illnesses caused by parasitic protozoan are a research priority. A representative group of these illnesses is the commonly known as Neglected Tropical Diseases (NTD). NTD specially attack low socioeconomic population around the world and new anti-protozoan inhibitors are needed and several drug discovery projects focus on researching new drug targets. Pharmacophylogenomics is a novel bioinformatics field that aims at reducing the time and the financial cost of the drug discovery process. Pharmacophylogenomic analyses are applied mainly in the early stages of the research phase in drug discovery. Pharmacophylogenomic analysis executes several bioinformatics programs in a coherent flow to identify homologues sequences, construct phylogenetic trees and execute evolutionary and structural experiments. This way, it can be modeled as scientific workflows. Pharmacophylogenomic analysis workflows are complex, computing and data intensive and may execute during weeks. This way, it benefits from parallel execution. We propose SciPPGx, a scientific workflow that aims at providing thorough inferring support for pharmacophylogenomic hypotheses. SciPPGx is executed in parallel in a cloud using SciCumulus workflow engine. Experiments show that SciPPGx considerably reduces the total execution time up to 97.1{\%} when compared to a sequential execution. We also present representative biological results taking advantage of the inference covering several related bioinformatics overviews. {\textcopyright}2012 IEEE.},
	keywords     = {Clouds,Pharmacophylogenomics,Scientific workflows}
}
@article{Ocana2012b,
	title        = {{Exploring molecular evolution reconstruction using a parallel cloud based scientific workflow}},
	author       = {Oca{\~{n}}a, Kary A.C.S. and {De Oliveira}, Daniel and Horta, Felipe and Dias, Jonas and Ogasawara, Eduardo and Mattoso, Marta},
	year         = 2012,
	journal      = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume       = {7409 LNBI},
	pages        = {179--191},
	doi          = {10.1007/978-3-642-31927-3_16},
	isbn         = 9783642319266,
	issn         = {03029743},
	abstract     = {Recent studies of evolution at molecular level address two important issues: reconstruction of the evolutionary relationships between species and investigation of the forces of the evolutionary process. Both issues experienced an explosive growth in the last two decades due to massive generation of genomic data, novel statistical methods and computational approaches to process and analyze this large volume of data. Most experiments in molecular evolution are based on computing intensive simulations preceded by other computation tools and post-processed by computing validators. All these tools can be modeled as scientific workflows to improve the experiment management while capturing provenance data. However, these evolutionary analyses experiments are very complex and may execute for weeks. These workflows need to be executed in parallel in High Performance Computing (HPC) environments such as clouds. Clouds are becoming adopted for bioinformatics experiments due to its characteristics, such as, elasticity and availability. Clouds are evolving into HPC environments. In this paper, we introduce SciEvol, a bioinformatics scientific workflow for molecular evolution reconstruction that aims at inferring evolutionary relationships (i.e. to detect positive Darwinian selection) on genomic data. SciEvol is designed and implemented to execute in parallel over the clouds using SciCumulus workflow engine. Our experiments show that SciEvol can help scientists by enabling the reconstruction of evolutionary relationships using the cloud environment. Results present performance improvements of up to 94.64{\%} in the execution time when compared to the sequential execution, which drops from around 10 days to 12 hours. {\textcopyright} 2012 Springer-Verlag.},
	keywords     = {Cloud,Molecular Evolution Reconstruction,Scientific Workflow}
}
@article{McMurdie2013,
	title        = {{Phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data}},
	author       = {McMurdie, Paul J. and Holmes, Susan},
	year         = 2013,
	journal      = {PLoS ONE},
	volume       = 8,
	number       = 4,
	doi          = {10.1371/journal.pone.0061217},
	isbn         = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
	issn         = 19326203,
	abstract     = {BACKGROUND: the analysis of microbial communities through dna sequencing brings many challenges: the integration of different types of data with methods from ecology, genetics, phylogenetics, multivariate statistics, visualization and testing. With the increased breadth of experimental designs now being pursued, project-specific statistical analyses are often needed, and these analyses are often difficult (or impossible) for peer researchers to independently reproduce. The vast majority of the requisite tools for performing these analyses reproducibly are already implemented in R and its extensions (packages), but with limited support for high throughput microbiome census data.$\backslash$n$\backslash$nRESULTS: Here we describe a software project, phyloseq, dedicated to the object-oriented representation and analysis of microbiome census data in R. It supports importing data from a variety of common formats, as well as many analysis techniques. These include calibration, filtering, subsetting, agglomeration, multi-table comparisons, diversity analysis, parallelized Fast UniFrac, ordination methods, and production of publication-quality graphics; all in a manner that is easy to document, share, and modify. We show how to apply functions from other R packages to phyloseq-represented data, illustrating the availability of a large number of open source analysis techniques. We discuss the use of phyloseq with tools for reproducible research, a practice common in other fields but still rare in the analysis of highly parallel microbiome census data. We have made available all of the materials necessary to completely reproduce the analysis and figures included in this article, an example of best practices for reproducible research.$\backslash$n$\backslash$nCONCLUSIONS: The phyloseq project for R is a new open-source software package, freely available on the web from both GitHub and Bioconductor.},
	archiveprefix = {arXiv},
	arxivid      = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
	eprint       = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
	pmid         = 23630581,
	primaryclass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:}
}
@article{McLaren2019,
	title        = {{Consistent and correctable bias in metagenomic sequencing experiments}},
	author       = {McLaren, Michael R and Willis, Amy D and Callahan, Benjamin J},
	year         = 2019,
	month        = sep,
	journal      = {eLife},
	publisher    = {eLife Sciences Publications, Ltd},
	volume       = 8,
	pages        = {e46923},
	doi          = {10.7554/eLife.46923},
	issn         = {2050-084X},
	url          = {https://doi.org/10.7554/eLife.46923},
	abstract     = {Marker-gene and metagenomic sequencing have profoundly expanded our ability to measure biological communities. But the measurements they provide differ from the truth, often dramatically, because these experiments are biased toward detecting some taxa over others. This experimental bias makes the taxon or gene abundances measured by different protocols quantitatively incomparable and can lead to spurious biological conclusions. We propose a mathematical model for how bias distorts community measurements based on the properties of real experiments. We validate this model with 16S rRNA gene and shotgun metagenomics data from defined bacterial communities. Our model better fits the experimental data despite being simpler than previous models. We illustrate how our model can be used to evaluate protocols, to understand the effect of bias on downstream statistical analyses, and to measure and correct bias given suitable calibration controls. These results illuminate new avenues toward truly quantitative and reproducible metagenomics measurements.},
	editor       = {Turnbaugh, Peter and Garrett, Wendy S and Turnbaugh, Peter and Quince, Christopher and Gibbons, Sean},
	keywords     = {16S rRNA gene,bias,calibration,metagenomics,microbiome,reproducibility}
}
@article{Jackson2020,
	title        = {{Using rapid prototyping to choose a bioinformatics workflow management system}},
	author       = {Jackson, Michael J and Wallace, Edward and Kavoussanakis, Kostas},
	year         = 2020,
	journal      = {bioRxiv},
	pages        = {2020.08.04.236208},
	url          = {https://doi.org/10.1101/2020.08.04.236208},
	abstract     = 10
}
@misc{Vivian2017,
	title        = {{Toil enables reproducible, open source, big biomedical data analyses}},
	author       = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D. and Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W. James and Patterson, David A. and Joseph, Anthony D. and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
	year         = 2017,
	booktitle    = {Nature Biotechnology},
	volume       = 35,
	number       = 4,
	pages        = {314--316},
	doi          = {10.1038/nbt.3772},
	issn         = 15461696,
	pmid         = 28398314
}
@mastersthesis{Pina2020,
	title        = {{An{\'{a}}lise de hiperpar{\^{a}}metros no treinamento de redes de aprendizado profundo usando dados de proveni{\^{e}}ncia}},
	author       = {Pina, D{\'{e}}bora Barbosa},
	year         = 2020,
	isbn         = 9789896540821,
	keywords     = {Aprendizado Profundo,Hiperpar{\^{a}}metros,Redes Neurais Convolucionais},
	school       = {Universidade Federal do Rio de Janeiro}
}
@article{Gu2018,
	title        = {{Recent advances in convolutional neural networks}},
	author       = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	year         = 2018,
	journal      = {Pattern Recognition},
	volume       = 77,
	pages        = {354--377},
	doi          = {https://doi.org/10.1016/j.patcog.2017.10.013},
	issn         = {0031-3203},
	url          = {http://www.sciencedirect.com/science/article/pii/S0031320317304120},
	abstract     = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
	keywords     = {Deep learning,Convolutional neural network}
}
@inproceedings{Pina2019,
	title        = {Análise de Hiperparâmetros em Aplicações de Aprendizado Profundo por meio de Dados de Proveniência},
	author       = {Débora Pina and Liliane Neves and Aline Paes and Daniel de Oliveira and Marta Mattoso},
	year         = 2019,
	booktitle    = {Anais do XXXIV Simpósio Brasileiro de Banco de Dados},
	location     = {Fortaleza},
	publisher    = {SBC},
	address      = {Porto Alegre, RS, Brasil},
	pages        = {223--228},
	doi          = {10.5753/sbbd.2019.8827},
	issn         = {0000-0000},
	url          = {https://sol.sbc.org.br/index.php/sbbd/article/view/8827},
	keywords     = {}
}
@article{amstutz2016common,
	title        = {{Common Workflow Language, v1.0}},
	author       = {Amstutz, Peter and Crusoe, Michael R and Tijani{\'{c}}, Neboj{\v{s}}a and Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and M{\'{e}}nager, Herv{\'{e}} and Nedeljkovich, Maya and Others},
	year         = 2016,
	publisher    = {Figshare},
	doi          = {https://doi.org/10.6084/m9.figshare.3115156.v2}
}
@misc{Gloor2017,
	title        = {{Microbiome datasets are compositional: And this is not optional}},
	author       = {Gloor, Gregory B. and Macklaim, Jean M. and Pawlowsky-Glahn, Vera and Egozcue, Juan J.},
	year         = 2017,
	booktitle    = {Frontiers in Microbiology},
	volume       = 8,
	number       = {NOV},
	doi          = {10.3389/fmicb.2017.02224},
	issn         = {1664302X},
	abstract     = {Datasets collected by high-throughput sequencing (HTS) of 16S rRNA gene amplimers, metagenomes or metatranscriptomes are commonplace and being used to study human disease states, ecological differences between sites, and the built environment. There is increasing awareness that microbiome datasets generated by HTS are compositional because they have an arbitrary total imposed by the instrument. However, many investigators are either unaware of this or assume specific properties of the compositional data. The purpose of this review is to alert investigators to the dangers inherent in ignoring the compositional nature of the data, and point out that HTS datasets derived from microbiome studies can and should be treated as compositions at all stages of analysis. We briefly introduce compositional data, illustrate the pathologies that occur when compositional data are analyzed inappropriately, and finally give guidance and point to resources and examples for the analysis of microbiome datasets using compositional data analysis.},
	keywords     = {Bayesian estimation,Compositional data,Correlation,Count normalization,High-throughput sequencing,Microbiota,Relative abundance}
}
@article{burrows1994,
	title        = {{A block-sorting lossless data compression algorithm}},
	author       = {Burrows, Michael and Wheeler, David J},
	year         = 1994,
	publisher    = {Citeseer}
}
@book{Stevens2013,
	title        = {Life out of sequence: a data-driven history of bioinformatics},
	author       = {Stevens, Hallam},
	year         = 2013,
	publisher    = {The University of Chicago Press}
}
https://www.britannica.com/science/bioinformatics
@article{Liew2017,
	title        = {{Scientific Workflows: Moving Across Paradigms}},
	author       = {Liew, Chee Sun and Atkinson, Malcolm P. and Galea, Michelle and Ang, Tan Fong and Martin, Paul and Hemert, Jano I. Van},
	year         = 2017,
	journal      = {ACM Computing Surveys},
	volume       = 49,
	number       = 4,
	pages        = {1--39},
	doi          = {10.1145/3012429},
	issn         = {0360-0300}
}
@article{Ponce2019,
	title        = {{Bridging the Educational Gap between Emerging and Established Scientific Computing Disciplines}},
	author       = {Ponce, Marcelo and Spence, Erik and van Zon, Ramses and Gruner, Daniel},
	year         = 2019,
	journal      = {The Journal of Computational Science Education},
	volume       = 10,
	number       = 1,
	pages        = {4--11},
	doi          = {10.22369/issn.2153-4136/10/1/1},
	issn         = {2153-4136},
	abstract     = {In this paper we describe our experience in developing curriculum courses aimed at graduate students in emerging computational fields, including biology and medical science. We focus primarily on computational data analysis and statistical analysis, while at the same time teaching students best practices in coding and software development. Our approach combines a theoretical background and practical applications of concepts. The outcomes and feedback we have obtained so far have revealed several issues: students in these particular areas lack instruction like this although they would tremendously benefit from it; we have detected several weaknesses in the formation of students, in particular in the statistical foundations but also in analytical thinking skills. We present here the tools, techniques and methodology we employ while teaching and developing this type of courses. We also show several outcomes from this initiative, including potential pathways for fruitful multi-disciplinary collaborations.},
	archiveprefix = {arXiv},
	arxivid      = {1901.05484},
	eprint       = {1901.05484},
	keywords     = {all or part of,computational statistics,curricula,graduate courses,or,or hard copies of,permission to make digital,student assessment,this work for personal,training and education}
}
@article{Brooksbank2014,
	title        = {{Bioinformatics Curriculum Guidelines: Toward a Definition of Core Competencies}},
	author       = {Brooksbank, Cath and Schneider, Maria Victoria and Radivojac, Predrag and Lewitter, Fran and Welch, Lonnie and Gaeta, Bruno and Schwartz, Russell},
	year         = 2014,
	journal      = {PLoS Computational Biology},
	volume       = 10,
	number       = 3,
	pages        = {e1003496},
	doi          = {10.1371/journal.pcbi.1003496}
}
@article{Chaumeil2019,
	title        = {{GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database}},
	author       = {Chaumeil, Pierre-Alain and Mussig, Aaron J and Hugenholtz, Philip and Parks, Donovan H},
	year         = 2019,
	journal      = {Bioinformatics},
	volume       = 36,
	number       = 6,
	pages        = {1925--1927},
	doi          = {10.1093/bioinformatics/btz848},
	issn         = {1367-4803},
	url          = {https://academic.oup.com/bioinformatics/article-abstract/36/6/1925/5626182},
	abstract     = {The GTDB Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the Genome Taxonomy Database (GTDB). GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10,156 bacterial and archaeal metagenome-assembled genomes.GTDB-Tk is implemented in Python and licensed under the GNU General Public License v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtkSupplementary data are available at Bioinformatics online.}
}
@article{Parks2018,
	title        = {{A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life.}},
	author       = {Parks, Donovan H and Chuvochina, Maria and Waite, David W and Rinke, Christian and Skarshewski, Adam and Chaumeil, Pierre-Alain and Hugenholtz, Philip},
	year         = 2018,
	journal      = {Nature biotechnology},
	volume       = 36,
	number       = 10,
	pages        = {996--1004},
	doi          = {10.1038/nbt.4229},
	issn         = {1546-1696},
	url          = {http://www.ncbi.nlm.nih.gov/pubmed/30148503},
	abstract     = {Taxonomy is an organizing principle of biology and is ideally based on evolutionary relationships among organisms. Development of a robust bacterial taxonomy has been hindered by an inability to obtain most bacteria in pure culture and, to a lesser extent, by the historical use of phenotypes to guide classification. Culture-independent sequencing technologies have matured sufficiently that a comprehensive genome-based taxonomy is now possible. We used a concatenated protein phylogeny as the basis for a bacterial taxonomy that conservatively removes polyphyletic groups and normalizes taxonomic ranks on the basis of relative evolutionary divergence. Under this approach, 58{\%} of the 94,759 genomes comprising the Genome Taxonomy Database had changes to their existing taxonomy. This result includes the description of 99 phyla, including six major monophyletic units from the subdivision of the Proteobacteria, and amalgamation of the Candidate Phyla Radiation into a single phylum. Our taxonomy should enable improved classification of uncultured bacteria and provide a sound basis for ecological and evolutionary studies.},
	pmid         = 30148503
}
@article{Breitwieser2018,
	title        = {{A review of methods and databases for metagenomic classification and assembly}},
	author       = {Breitwieser, Florian P. and Lu, Jennifer and Salzberg, Steven L.},
	year         = 2018,
	journal      = {Briefings in Bioinformatics},
	volume       = 20,
	number       = 4,
	pages        = {1125--1139},
	doi          = {10.1093/bib/bbx120},
	issn         = 14774054,
	keywords     = {bacteria,databases,microbial genomics,microbiome,next-generation sequencing}
}
@article{Mardis2011,
	title        = {{A decade's perspective on DNA sequencing technology}},
	author       = {Mardis, Elaine R.},
	year         = 2011,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 470,
	number       = 7333,
	pages        = {198--203},
	doi          = {10.1038/nature09796},
	isbn         = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
	issn         = {00280836},
	url          = {http://dx.doi.org/10.1038/nature09796},
	abstract     = {The decade since the Human Genome Project ended has witnessed a remarkable sequencing technology explosion that has permitted a multitude of questions about the genome to be asked and answered, at unprecedented speed and resolution. Here I present examples of how the resulting information has both enhanced our knowledge and expanded the impact of the genome on biomedical research. New sequencing technologies have also introduced exciting new areas of biological endeavour. The continuing upward trajectory of sequencing technology development is enabling clinical applications that are aimed at improving medical diagnosis and treatment.},
	pmid         = 21307932
}
@article{Gilbert2018,
	title        = {{Earth Microbiome Project and Global Systems Biology}},
	author       = {Gilbert, Jack A. and Jansson, Janet K. and Knight, Rob},
	year         = 2018,
	journal      = {mSystems},
	volume       = 3,
	number       = 3,
	pages        = {1--4},
	doi          = {10.1128/msystems.00217-17},
	issn         = {2379-5077},
	keywords     = {1,2,43 countries,751 samples acquired from,a truly multidisciplinary research,amplicon,and 27,biome project,ecently,emp,from the earth micro-,global,large-scale analysis of data,meta-analysis,metadata,microbiome,more than 500 scientists,program involving,systems biology,these,we published the first}
}
@article{Sherman2020,
	title        = {{Pan-genomics in the human genome era}},
	author       = {Sherman, Rachel M. and Salzberg, Steven L.},
	year         = 2020,
	journal      = {Nature Reviews Genetics},
	publisher    = {Springer US},
	volume       = 21,
	number       = 4,
	pages        = {243--254},
	doi          = {10.1038/s41576-020-0210-7},
	issn         = 14710064,
	url          = {http://dx.doi.org/10.1038/s41576-020-0210-7},
	abstract     = {Since the early days of the genome era, the scientific community has relied on a single ‘reference' genome for each species, which is used as the basis for a wide range of genetic analyses, including studies of variation within and across species. As sequencing costs have dropped, thousands of new genomes have been sequenced, and scientists have come to realize that a single reference genome is inadequate for many purposes. By sampling a diverse set of individuals, one can begin to assemble a pan-genome: a collection of all the DNA sequences that occur in a species. Here we review efforts to create pan-genomes for a range of species, from bacteria to humans, and we further consider the computational methods that have been proposed in order to capture, interpret and compare pan-genome data. As scientists continue to survey and catalogue the genomic variation across human populations and begin to assemble a human pan-genome, these efforts will increase our power to connect variation to human diversity, disease and beyond.},
	pmid         = 32034321
}
@article{Scholz2012,
	title        = {{Next generation sequencing and bioinformatic bottlenecks: The current state of metagenomic data analysis}},
	author       = {Scholz, Matthew B. and Lo, Chien Chi and Chain, Patrick S G},
	year         = 2012,
	journal      = {Current Opinion in Biotechnology},
	volume       = 23,
	number       = 1,
	pages        = {9--15},
	doi          = {10.1016/j.copbio.2011.11.013},
	issn         = {09581669},
	abstract     = {The recent technological advances in next generation sequencing have brought the field closer to the goal of reconstructing all genomes within a community by presenting high throughput sequencing at much lower costs. While these next-generation sequencing technologies have allowed a massive increase in available raw sequence data, there are a number of new informatics challenges and difficulties that must be addressed to improve the current state, and fulfill the promise of, metagenomics. {\textcopyright} 2011 Elsevier Ltd.}
}
@article{Luciano2007,
	title        = {{E-Science and biological pathway semantics}},
	author       = {Luciano, Joanne S. and Stevens, Robert D.},
	year         = 2007,
	journal      = {BMC Bioinformatics},
	volume       = 8,
	number       = {SUPPL. 3},
	doi          = {10.1186/1471-2105-8-S3-S3},
	issn         = 14712105,
	abstract     = {Background: The development of e-Science presents a major set of opportunities and challenges for the future progress of biological and life scientific research. Major new tools are required and corresponding demands are placed on the high-throughput data generated and used in these processes. Nowhere is the demand greater than in the semantic integration of these data. Semantic Web tools and technologies afford the chance to achieve this semantic integration. Since pathway knowledge is central to much of the scientific research today it is a good test-bed for semantic integration. Within the context of biological pathways, the BioPAX initiative, part of a broader movement towards the standardization and integration of life science databases, forms a necessary prerequisite for its successful application of e-Science in health care and life science research. This paper examines whether BioPAX, an effort to overcome the barrier of disparate and heterogeneous pathway data sources, addresses the needs of e-Science. Results: We demonstrate how BioPAX pathway data can be used to ask and answer some useful biological questions. We find that BioPAX comes close to meeting a broad range of e-Science needs, but certain semantic weaknesses mean that these goals are missed. We make a series of recommendations for re-modeling some aspects of BioPAX to better meet these needs. Conclusion: Once these semantic weaknesses are addressed, it will be possible to integrate pathway information in a manner that would be useful in e-Science. {\textcopyright} 2007 Luciano and Stevens; licensee BioMed Central Ltd.},
	pmid         = 17493286
}
@article{Edwards2016,
	title        = {{Reproducibility: Team up with industry}},
	author       = {Edwards, Aled},
	year         = 2016,
	journal      = {Nature},
	volume       = 531,
	number       = 7594,
	pages        = {299--301},
	doi          = {10.1038/531299a},
	issn         = {0028-0836},
	abstract     = {8 reglas de oro para el ��xito en la colaboraci��n con la industria. Al final, si promueves el open access y la no protecci��n de resultados (sin pasarse de primo) es bueno, las empresas acuden m��s a ti porque saben que f��an m��s, eres m��s reproducible y menos oscuro y a la larga lo que pierdes de no patentar te llegar�� de proyectos en los que ellos te busquen para colaborar.}
}
@article{Wilkinson2016,
	title        = {{The FAIR Guiding Principles for scientific data management and stewardship}},
	author       = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`{e}} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and t Hoen, Peter A.C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {Van Der Lei}, Johan and {Van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	year         = 2016,
	journal      = {Scientific Data},
	volume       = 3,
	pages        = {1--9},
	doi          = {10.1038/sdata.2016.18},
	issn         = 20524463,
	abstract     = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.}
}
@article{Nekrutenko2012,
	title        = {{Next-generation sequencing data interpretation: Enhancing reproducibility and accessibility}},
	author       = {Nekrutenko, Anton and Taylor, James},
	year         = 2012,
	journal      = {Nature Reviews Genetics},
	publisher    = {Nature Publishing Group},
	volume       = 13,
	number       = 9,
	pages        = {667--672},
	doi          = {10.1038/nrg3305},
	issn         = 14710056,
	url          = {http://dx.doi.org/10.1038/nrg3305},
	abstract     = {Areas of life sciences research that were previously distant from each other in ideology, analysis practices and toolkits, such as microbial ecology and personalized medicine, have all embraced techniques that rely on next-generation sequencing instruments. Yet the capacity to generate the data greatly outpaces our ability to analyse it. Existing sequencing technologies are more mature and accessible than the methodologies that are available for individual researchers to move, store, analyse and present data in a fashion that is transparent and reproducible. Here we discuss currently pressing issues with analysis, interpretation, reproducibility and accessibility of these data, and we present promising solutions and venture into potential future developments.}
}
@article{Peng2015,
	title        = {{The Reproducibility Crisis in Science}},
	author       = {Peng, Roger},
	year         = 2015,
	journal      = {Significance},
	volume       = {June},
	pages        = {30--32},
	doi          = {10.1111/j.1740-9713.2015.00827.x},
	issn         = 17409705,
	abstract     = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
	pmid         = 29695587
}
@article{Miles2007,
	title        = {{Provenance-based validation of e-science experiments}},
	author       = {Miles, Simon and Wong, Sylvia C. and Fang, Weijian and Groth, Paul and Zauner, Klaus Peter and Moreau, Luc},
	year         = 2007,
	journal      = {Web Semantics},
	volume       = 5,
	number       = 1,
	pages        = {28--38},
	doi          = {10.1016/j.websem.2006.11.003},
	issn         = 15708268,
	abstract     = {E-science experiments typically involve many distributed services maintained by different organisations. After an experiment has been executed, it is useful for a scientist to verify that the execution was performed correctly or is compatible with some existing experimental criteria or standards, not necessarily anticipated prior to execution. Scientists may also want to review and verify experiments performed by their colleagues. There are no existing frameworks for validating such experiments in today's e-science systems. Users therefore have to rely on error checking performed by the services, or adopt other ad hoc methods. This paper introduces a platform-independent framework for validating workflow executions. The validation relies on reasoning over the documented provenance of experiment results and semantic descriptions of services advertised in a registry. This validation process ensures experiments are performed correctly, and thus results generated are meaningful. The framework is tested in a bioinformatics application that performs protein compressibility analysis. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	keywords     = {E-science,Process validation,Provenance,Semantic service description}
}
@article{Papageorgiou2018,
	title        = {{Genomic big data hitting the storage bottleneck.}},
	author       = {Papageorgiou, Louis and Eleni, Picasi and Raftopoulou, Sofia and Mantaiou, Meropi and Megalooikonomou, Vasileios and Vlachakis, Dimitrios},
	year         = 2018,
	journal      = {EMBnet.journal},
	volume       = 24,
	issn         = {2226-6089},
	url          = {http://www.ncbi.nlm.nih.gov/pubmed/29782620{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5958914},
	abstract     = {During the last decades, there is a vast data explosion in bioinformatics. Big data centres are trying to face this data crisis, reaching high storage capacity levels. Although several scientific giants examine how to handle the enormous pile of information in their cupboards, the problem remains unsolved. On a daily basis, there is a massive quantity of permanent loss of extensive information due to infrastructure and storage space problems. The motivation for sequencing has fallen behind. Sometimes, the time that is spent to solve storage space problems is longer than the one dedicated to collect and analyse data. To bring sequencing to the foreground, scientists have to slide over such obstacles and find alternative ways to approach the issue of data volume. Scientific community experiences the data crisis era, where, out of the box solutions may ease the typical research workflow, until technological development meets the needs of Bioinformatics.},
	pmid         = 29782620
}
% Recomendações
@article{Oliveira2014,
	title        = {{Debugging Scientific Workflows with Provenance: Achievements and Lessons Learned}},
	author       = {Oliveira, Daniel De and Costa, Flavio and Silva, V{\'{i}}tor and Oca{\~{n}}a, Kary and Mattoso, Marta},
	year         = 2014,
	journal      = {Proc. of the SBBD - Brazilian Symp. on Databases},
	pages        = {67--76}
}
@article{Oliveira2018,
	title        = {{Provenance analytics for workflow-based computational experiments: A survey}},
	author       = {Oliveira, Wellington and {De Oliveira}, Daniel and Braganholo, Vanessa},
	year         = 2018,
	journal      = {ACM Computing Surveys},
	volume       = 51,
	number       = 3,
	doi          = {10.1145/3184900},
	issn         = 15577341,
	abstract     = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and ScientificWorkflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis.We also contribute by proposing a taxonomy to classify elements related to provenance analytics.},
	keywords     = {Data analytics,Provenance,Scientific experiments,Scientific workflows}
}
@article{Dodsworth2017,
	title        = {{Minimum information about a single amplified genome (MISAG) and a metagenome-assembled genome (MIMAG) of bacteria and archaea}},
	author       = {Dodsworth, Jeremy A and Hedlund, Brian and Malmstrom, Rex R and Schriml, Lynn and Rattei, Thomas and Hallam, Steven J and Rinke, Christian and Konstantinidis, Konstantinos T and Rivers, Adam R and Birren, Bruce and Podar, Mircea and Kyrpides, Nikos C and Nelson, William C and Weinstock, George M and Harmon-Smith, Miranda and Eisen, Jonathan A and Clum, Alicia and Tighe, Scott and Fierer, Noah and Eren, A M and Reddy, T B K and Eloe-Fadrosh, Emiley A and Gilbert, Jack A and Jarett, Jessica and Schulz, Frederik and Yilmaz, Pelin and Meyer, Folker and Ivanova, Natalia N and Knight, Rob and Hugenholtz, Philip and Banfield, Jillian F and Liu, Wen-Tso and McMahon, Katherine D and Becraft, Eric D and Jungbluth, Sean P and Karsch-Mizrachi, Ilene and Cochrane, Guy and Lapidus, Alla and Yooseph, Shibu and Copeland, Alex and Garrity, George M and Woyke, Tanja and Sutton, Granger and Finn, Rob and Tyson, Gene W and Doud, Devin and Parks, Donovan H and Tringe, Susannah G and Bowers, Robert M and Gl{\"{o}}ckner, Frank O and Ettema, Thijs J G and Bork, Peer and Baker, Brett J and Stepanauskas, Ramunas},
	year         = 2017,
	journal      = {Nature Biotechnology},
	volume       = 35,
	number       = 8,
	pages        = {725--731},
	doi          = {10.1038/nbt.3893},
	issn         = {1087-0156}
}
@article{Yilmaz2011,
	title        = {{Minimum information about a marker gene sequence (MIMARKS) and minimum information about any (x) sequence (MIxS) specifications}},
	author       = {Yilmaz, Pelin and Kottmann, Renzo and Field, Dawn and Knight, Rob and Cole, James R. and Amaral-Zettler, Linda and Gilbert, Jack A. and Karsch-Mizrachi, Ilene and Johnston, Anjanette and Cochrane, Guy and Vaughan, Robert and Hunter, Christopher and Park, Joonhong and Morrison, Norman and Rocca-Serra, Philippe and Sterk, Peter and Arumugam, Manimozhiyan and Bailey, Mark and Baumgartner, Laura and Birren, Bruce W. and Blaser, Martin J. and Bonazzi, Vivien and Booth, Tim and Bork, Peer and Bushman, Frederic D. and Buttigieg, Pier Luigi and Chain, Patrick S.G. and Charlson, Emily and Costello, Elizabeth K. and Huot-Creasy, Heather and Dawyndt, Peter and Desantis, Todd and Fierer, Noah and Fuhrman, Jed A. and Gallery, Rachel E. and Gevers, Dirk and Gibbs, Richard A. and Gil, Inigo San and Gonzalez, Antonio and Gordon, Jeffrey I. and Guralnick, Robert and Hankeln, Wolfgang and Highlander, Sarah and Hugenholtz, Philip and Jansson, Janet and Kau, Andrew L. and Kelley, Scott T. and Kennedy, Jerry and Knights, Dan and Koren, Omry and Kuczynski, Justin and Kyrpides, Nikos and Larsen, Robert and Lauber, Christian L. and Legg, Teresa and Ley, Ruth E. and Lozupone, Catherine A. and Ludwig, Wolfgang and Lyons, Donna and Maguire, Eamonn and Meth{\'{e}}, Barbara A. and Meyer, Folker and Muegge, Brian and Nakielny, Sara and Nelson, Karen E. and Nemergut, Diana and Neufeld, Josh D. and Newbold, Lindsay K. and Oliver, Anna E. and Pace, Norman R. and Palanisamy, Giriprakash and Peplies, J{\"{o}}rg and Petrosino, Joseph and Proctor, Lita and Pruesse, Elmar and Quast, Christian and Raes, Jeroen and Ratnasingham, Sujeevan and Ravel, Jacques and Relman, David A. and Assunta-Sansone, Susanna and Schloss, Patrick D. and Schriml, Lynn and Sinha, Rohini and Smith, Michelle I. and Sodergren, Erica and Spor, Aym{\'{e}} and Stombaugh, Jesse and Tiedje, James M. and Ward, Doyle V. and Weinstock, George M. and Wendel, Doug and White, Owen and Whiteley, Andrew and Wilke, Andreas and Wortman, Jennifer R. and Yatsunenko, Tanya and Gl{\"{o}}ckner, Frank Oliver},
	year         = 2011,
	journal      = {Nature Biotechnology},
	volume       = 29,
	number       = 5,
	pages        = {415--420},
	doi          = {10.1038/nbt.1823},
	issn         = 10870156,
	abstract     = {Here we present a standard developed by the Genomic Standards Consortium (GSC) for reporting marker gene sequences-the minimum information about a marker gene sequence (MIMARKS). We also introduce a system for describing the environment from which a biological sample originates. The 'environmental packages' apply to any genome sequence of known origin and can be used in combination with MIMARKS and other GSC checklists. Finally, to establish a unified standard for describing sequence data and to provide a single point of entry for the scientific community to access and learn about GSC checklists, we present the minimum information about any (x) sequence (MIxS). Adoption of MIxS will enhance our ability to analyze natural genetic diversity documented by massive DNA sequencing efforts from myriad ecosystems in our ever-changing biosphere.},
	pmid         = 21552244
}
@article{Field2008,
	title        = {{The minimum information about a genome sequence (MIGS) specification}},
	author       = {Field, Dawn and Garrity, George and Gray, Tanya and Morrison, Norman and Selengut, Jeremy and Sterk, Peter and Tatusova, Tatiana and Thomson, Nicholas and Allen, Michael J and Angiuoli, Samuel V and Ashburner, Michael and Axelrod, Nelson and Baldauf, Sandra and Ballard, Stuart and Boore, Jeffrey and Cochrane, Guy and Cole, James and Dawyndt, Peter and {De Vos}, Paul and DePamphilis, Claude and Edwards, Robert and Faruque, Nadeem and Feldman, Robert and Gilbert, Jack and Gilna, Paul and Gl{\"{o}}ckner, Frank Oliver and Goldstein, Philip and Guralnick, Robert and Haft, Dan and Hancock, David and Hermjakob, Henning and Hertz-Fowler, Christiane and Hugenholtz, Phil and Joint, Ian and Kagan, Leonid and Kane, Matthew and Kennedy, Jessie and Kowalchuk, George and Kottmann, Renzo and Kolker, Eugene and Kravitz, Saul and Kyrpides, Nikos and Leebens-Mack, Jim and Lewis, Suzanna E and Li, Kelvin and Lister, Allyson L and Lord, Phillip and Maltsev, Natalia and Markowitz, Victor and Martiny, Jennifer and Methe, Barbara and Mizrachi, Ilene and Moxon, Richard and Nelson, Karen and Parkhill, Julian and Proctor, Lita and White, Owen and Sansone, Susanna-Assunta and Spiers, Andrew and Stevens, Robert and Swift, Paul and Taylor, Chris and Tateno, Yoshio and Tett, Adrian and Turner, Sarah and Ussery, David and Vaughan, Bob and Ward, Naomi and Whetzel, Trish and {San Gil}, Ingio and Wilson, Gareth and Wipat, Anil},
	year         = 2008,
	month        = may,
	journal      = {Nature Biotechnology},
	publisher    = {Nature Publishing Group},
	volume       = 26,
	pages        = 541,
	url          = {https://doi.org/10.1038/nbt1360 http://10.0.4.14/nbt1360 https://www.nature.com/articles/nbt1360{\#}supplementary-information}
}
